{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Latent Tree experiments using DLGN**","metadata":{}},{"cell_type":"markdown","source":"**Datasets for Latent Tree experiments**","metadata":{}},{"cell_type":"code","source":"'''\n# Use the data in DLGN and its variations from https://arxiv.org/abs/2010.04627\n'''\n#Imports for Latent Tree data\n\nimport random\nimport requests\nimport os\nimport torch\nfrom tqdm import tqdm\nimport numpy as np\nimport gzip\nimport shutil\nimport tarfile\nimport bz2\nimport pandas as pd\nimport gzip\nimport shutil\nimport warnings\n\nfrom pathlib import Path\nfrom sklearn.datasets import load_svmlight_file\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_swiss_roll\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom category_encoders import LeaveOneOutEncoder\nfrom category_encoders.ordinal import OrdinalEncoder\nimport os\nimport zipfile\nimport shutil\nimport urllib.request\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom scipy.io import arff\n# !pip install numpy==1.22.0  # Install a compatible version of NumPy\n# !pip install scipy  # Install or update SciPy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import pairwise_distances\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom itertools import product as cartesian_prod\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import pairwise_distances\n\nfrom sklearn import tree\nfrom sklearn import cluster, mixture\n\n\nnp.set_printoptions(precision=4)\n\n\n#@title Importing Packages\nimport os\nimport random\nimport pandas as pd\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport time\nimport sys\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:36.727754Z","iopub.execute_input":"2023-09-28T20:47:36.728127Z","iopub.status.idle":"2023-09-28T20:47:42.649266Z","shell.execute_reply.started":"2023-09-28T20:47:36.728097Z","shell.execute_reply":"2023-09-28T20:47:42.648258Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"#**from src.utils import download**\ndef download(url, filename, delete_if_interrupted=True, chunk_size=4096):\n    \"\"\" saves file from url to filename with a fancy progressbar \"\"\"\n    try:\n        with open(filename, \"wb\") as f:\n            print(\"Downloading {} > {}\".format(url, filename))\n            response = requests.get(url, stream=True)\n            total_length = response.headers.get('content-length')\n\n            if total_length is None:  # no content length header\n                f.write(response.content)\n            else:\n                total_length = int(total_length)\n                with tqdm(total=total_length) as progressbar:\n                    for data in response.iter_content(chunk_size=chunk_size):\n                        if data:  # filter-out keep-alive chunks\n                            f.write(data)\n                            progressbar.update(len(data))\n\n    except Exception as e:\n        if delete_if_interrupted:\n            print(\"Removing incomplete download {}.\".format(filename))\n            os.remove(filename)\n        raise e\n    return filename\n\n#**from src.clus_datasets import ***\n#**CLUSTERING DATASET FETCHERS**\n\n# -------------------------------------------------------- CLUSTERING DATASET FETCHERS\n\ndef fetch_GLASS(path, valid_size=0.2, test_size=0.2, seed=None):\n\n    path = Path(path)\n    data_path = path / 'glass.data'\n\n    if not data_path.exists():\n        path.mkdir(parents=True)\n\n        download('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', data_path)\n\n    data = np.genfromtxt(data_path, delimiter=',')\n    \n    X, Y = (data[:, 1:-1]).astype(np.float32), (data[:, -1] - 1).astype(int)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=seed)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=valid_size / (1 - test_size), random_state=seed)\n\n    return dict(\n        X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test\n    )\n\ndef fetch_COVTYPE(path, valid_size=0.2, test_size=0.2, seed=None):\n\n    path = Path(path)\n    data_path = path / 'covtype.data'\n\n    if not data_path.exists():\n        path.mkdir(parents=True)\n        archive_path = path / 'covtype.data.gz'\n        download('https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz', archive_path)\n\n        with gzip.open(archive_path, 'rb') as f_in:\n\n            with open(data_path, 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n\n    data = np.genfromtxt(data_path, delimiter=',')\n    \n    X, Y = (data[:, :-1]).astype(np.float32), (data[:, -1] - 1).astype(int)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=seed)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=valid_size / (1 - test_size), random_state=seed)\n\n    return dict(\n        X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test\n    )\n\ndef fetch_ALOI(path, valid_size=0.2, test_size=0.2, seed=None):\n\n    from PIL import Image\n    from tqdm import tqdm\n\n    path = Path(path)\n    data_path = path / 'aloi_red4'\n    npz_path = path / 'aloi_red4.npz'\n\n    if not data_path.exists():\n\n        path.mkdir(parents=True, exist_ok=True)\n\n        for data_type in [\"ill\", \"col\", \"view\", \"stereo\"]:\n            \n            archive_path = path / f'aloi_red4_{data_type}.tar'\n\n            download(f'http://aloi.science.uva.nl/tars/aloi_red4_{data_type}.tar', archive_path)\n\n            with tarfile.open(archive_path, 'r') as f_in:\n\n                f_in.extractall(path=data_path)\n\n    if not npz_path.exists():\n        \n        X = np.empty((110250, 3, 144, 192), dtype=np.uint8)\n        Y = np.empty(110250, dtype=np.uint16)\n\n        # loop over classes\n        i = 0\n        for c in tqdm(range(1000), desc='Converting ALOI png to npy'):\n\n            c_path = data_path / \"png4\" / str(c + 1) \n\n            # loop over class instances\n            for i_path in c_path.glob('*.png'):\n                \n                im_frame = Image.open(i_path)\n                X[i] = np.transpose(np.array(im_frame), (2, 0, 1))\n                Y[i] = c\n                i += 1\n\n        np.savez_compressed(npz_path, X=X, Y=Y)\n    \n    else:\n\n        data = np.load(npz_path)\n        X, Y = data['X'], data['Y']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=seed)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=valid_size / (1 - test_size), random_state=seed)\n\n    return dict(\n        X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test\n    )\n\ndef fetch_DIGITS(path, valid_size=0.2, test_size=0.2, seed=None):\n\n    from sklearn.datasets import load_digits\n\n    X, Y = load_digits(return_X_y=True)\n    X, Y = X.reshape(-1, 1, 8, 8).astype(np.uint8), Y.astype(int)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=seed)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=valid_size / (1 - test_size), random_state=seed)\n\n    return dict(\n        X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test\n    )\n\n#**from src.tabular_datasets import ***\n\ndef fetch_A9A(path, train_size=None, valid_size=None, test_size=None, **kwargs):\n    train_path = os.path.join(path, 'a9a')\n    test_path = os.path.join(path, 'a9a.t')\n    if not all(os.path.exists(fname) for fname in (train_path, test_path)):\n        os.makedirs(path, exist_ok=True)\n        download(\"https://www.dropbox.com/s/9cqdx166iwonrj9/a9a?dl=1\", train_path)\n        download(\"https://www.dropbox.com/s/sa0ds895c0v4xc6/a9a.t?dl=1\", test_path)\n\n    X_train, y_train = load_svmlight_file(train_path, dtype=np.float32, n_features=123)\n    X_test, y_test = load_svmlight_file(test_path, dtype=np.float32, n_features=123)\n    X_train, X_test = X_train.toarray(), X_test.toarray()\n    y_train[y_train == -1] = 0\n    y_test[y_test == -1] = 0\n    y_train, y_test = y_train.astype(np.int), y_test.astype(np.int)\n\n    if all(sizes is None for sizes in (train_size, valid_size, test_size)):\n        train_idx_path = os.path.join(path, 'stratified_train_idx.txt')\n        valid_idx_path = os.path.join(path, 'stratified_valid_idx.txt')\n        if not all(os.path.exists(fname) for fname in (train_idx_path, valid_idx_path)):\n            download(\"https://www.dropbox.com/s/xy4wwvutwikmtha/stratified_train_idx.txt?dl=1\", train_idx_path)\n            download(\"https://www.dropbox.com/s/nthpxofymrais5s/stratified_test_idx.txt?dl=1\", valid_idx_path)\n        train_idx = pd.read_csv(train_idx_path, header=None)[0].values\n        valid_idx = pd.read_csv(valid_idx_path, header=None)[0].values\n    else:\n        assert train_size, \"please provide either train_size or none of sizes\"\n        if valid_size is None:\n            valid_size = len(X_train) - train_size\n            assert valid_size > 0\n        if train_size + valid_size > len(X_train):\n            warnings.warn('train_size + valid_size = {} exceeds dataset size: {}.'.format(\n                train_size + valid_size, len(X_train)), Warning)\n        if test_size is not None:\n            warnings.warn('Test set is fixed for this dataset.', Warning)\n\n        shuffled_indices = np.random.permutation(np.arange(len(X_train)))\n        train_idx = shuffled_indices[:train_size]\n        valid_idx = shuffled_indices[train_size: train_size + valid_size]    \n\n    return dict(\n        X_train=X_train[train_idx], y_train=y_train[train_idx],\n        X_valid=X_train[valid_idx], y_valid=y_train[valid_idx],\n        X_test=X_test, y_test=y_test\n    )\n\ndef fetch_EPSILON(path, train_size=None, valid_size=None, test_size=None, **kwargs):\n    train_path = os.path.join(path, 'epsilon_normalized')\n    test_path = os.path.join(path, 'epsilon_normalized.t')\n    if not all(os.path.exists(fname) for fname in (train_path, test_path)):\n        os.makedirs(path, exist_ok=True)\n        train_archive_path = os.path.join(path, 'epsilon_normalized.bz2')\n        test_archive_path = os.path.join(path, 'epsilon_normalized.t.bz2')\n        if not all(os.path.exists(fname) for fname in (train_archive_path, test_archive_path)):\n            download(\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/epsilon_normalized.bz2\", train_archive_path)\n            download(\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/epsilon_normalized.t.bz2\", test_archive_path)\n        print(\"unpacking dataset\")\n        for file_name, archive_name in zip((train_path, test_path), (train_archive_path, test_archive_path)):\n            zipfile = bz2.BZ2File(archive_name)\n            with open(file_name, 'wb') as f:\n                f.write(zipfile.read())\n\n    print(\"reading dataset (it may take a long time)\")\n    X_train, y_train = load_svmlight_file(train_path, dtype=np.float32, n_features=2000)\n    X_test, y_test = load_svmlight_file(test_path, dtype=np.float32, n_features=2000)\n    X_train, X_test = X_train.toarray(), X_test.toarray()\n    y_train, y_test = y_train.astype(np.int), y_test.astype(np.int)\n    y_train[y_train == -1] = 0\n    y_test[y_test == -1] = 0\n\n    if all(sizes is None for sizes in (train_size, valid_size, test_size)):\n        train_idx_path = os.path.join(path, 'stratified_train_idx.txt')\n        valid_idx_path = os.path.join(path, 'stratified_valid_idx.txt')\n        if not all(os.path.exists(fname) for fname in (train_idx_path, valid_idx_path)):\n            download(\"https://www.dropbox.com/s/wxgm94gvm6d3xn5/stratified_train_idx.txt?dl=1\", train_idx_path)\n            download(\"https://www.dropbox.com/s/fm4llo5uucdglti/stratified_valid_idx.txt?dl=1\", valid_idx_path)\n        train_idx = pd.read_csv(train_idx_path, header=None)[0].values\n        valid_idx = pd.read_csv(valid_idx_path, header=None)[0].values\n    else:\n        assert train_size, \"please provide either train_size or none of sizes\"\n        if valid_size is None:\n            valid_size = len(X_train) - train_size\n            assert valid_size > 0\n        if train_size + valid_size > len(X_train):\n            warnings.warn('train_size + valid_size = {} exceeds dataset size: {}.'.format(\n                train_size + valid_size, len(X_train)), Warning)\n        if test_size is not None:\n            warnings.warn('Test set is fixed for this dataset.', Warning)\n\n        shuffled_indices = np.random.permutation(np.arange(len(X_train)))\n        train_idx = shuffled_indices[:train_size]\n        valid_idx = shuffled_indices[train_size: train_size + valid_size]\n\n    return dict(\n        X_train=X_train[train_idx], y_train=y_train[train_idx],\n        X_valid=X_train[valid_idx], y_valid=y_train[valid_idx],\n        X_test=X_test, y_test=y_test\n    )\n\n\ndef fetch_PROTEIN(path, train_size=None, valid_size=None, test_size=None, **kwargs):\n    \"\"\"\n    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#protein\n    \"\"\"\n    train_path = os.path.join(path, 'protein')\n    test_path = os.path.join(path, 'protein.t')\n    if not all(os.path.exists(fname) for fname in (train_path, test_path)):\n        os.makedirs(path, exist_ok=True)\n        download(\"https://www.dropbox.com/s/pflp4vftdj3qzbj/protein.tr?dl=1\", train_path)\n        download(\"https://www.dropbox.com/s/z7i5n0xdcw57weh/protein.t?dl=1\", test_path)\n    for fname in (train_path, test_path):\n        raw = open(fname).read().replace(' .', '0.')\n        with open(fname, 'w') as f:\n            f.write(raw)\n\n    X_train, y_train = load_svmlight_file(train_path, dtype=np.float32, n_features=357)\n    X_test, y_test = load_svmlight_file(test_path, dtype=np.float32, n_features=357)\n    X_train, X_test = X_train.toarray(), X_test.toarray()\n    y_train, y_test = y_train.astype(np.int), y_test.astype(np.int)\n\n    if all(sizes is None for sizes in (train_size, valid_size, test_size)):\n        train_idx_path = os.path.join(path, 'stratified_train_idx.txt')\n        valid_idx_path = os.path.join(path, 'stratified_valid_idx.txt')\n        if not all(os.path.exists(fname) for fname in (train_idx_path, valid_idx_path)):\n            download(\"https://www.dropbox.com/s/wq2v9hl1wxfufs3/small_stratified_train_idx.txt?dl=1\", train_idx_path)\n            download(\"https://www.dropbox.com/s/7o9el8pp1bvyy22/small_stratified_valid_idx.txt?dl=1\", valid_idx_path)\n        train_idx = pd.read_csv(train_idx_path, header=None)[0].values\n        valid_idx = pd.read_csv(valid_idx_path, header=None)[0].values\n    else:\n        assert train_size, \"please provide either train_size or none of sizes\"\n        if valid_size is None:\n            valid_size = len(X_train) - train_size\n            assert valid_size > 0\n        if train_size + valid_size > len(X_train):\n            warnings.warn('train_size + valid_size = {} exceeds dataset size: {}.'.format(\n                train_size + valid_size, len(X_train)), Warning)\n        if test_size is not None:\n            warnings.warn('Test set is fixed for this dataset.', Warning)\n\n        shuffled_indices = np.random.permutation(np.arange(len(X_train)))\n        train_idx = shuffled_indices[:train_size]\n        valid_idx = shuffled_indices[train_size: train_size + valid_size]\n\n    return dict(\n        X_train=X_train[train_idx], y_train=y_train[train_idx],\n        X_valid=X_train[valid_idx], y_valid=y_train[valid_idx],\n        X_test=X_test, y_test=y_test\n    )\n\n\ndef fetch_YEAR(path, train_size=None, valid_size=None, test_size=51630, **kwargs):\n    data_path = os.path.join(path, 'data.csv')\n    if not os.path.exists(data_path):\n        os.makedirs(path, exist_ok=True)\n        download('https://www.dropbox.com/s/l09pug0ywaqsy0e/YearPredictionMSD.txt?dl=1', data_path)\n    n_features = 91\n    types = {i: (np.float32 if i != 0 else np.int) for i in range(n_features)}\n    data = pd.read_csv(data_path, header=None, dtype=types)\n    data_train, data_test = data.iloc[:-test_size], data.iloc[-test_size:]\n\n    X_train, y_train = data_train.iloc[:, 1:].values, data_train.iloc[:, 0].values\n    X_test, y_test = data_test.iloc[:, 1:].values, data_test.iloc[:, 0].values\n\n    if all(sizes is None for sizes in (train_size, valid_size)):\n        train_idx_path = os.path.join(path, 'stratified_train_idx.txt')\n        valid_idx_path = os.path.join(path, 'stratified_valid_idx.txt')\n        if not all(os.path.exists(fname) for fname in (train_idx_path, valid_idx_path)):\n            download(\"https://www.dropbox.com/s/00u6cnj9mthvzj1/stratified_train_idx.txt?dl=1\", train_idx_path)\n            download(\"https://www.dropbox.com/s/420uhjvjab1bt7k/stratified_valid_idx.txt?dl=1\", valid_idx_path)\n        train_idx = pd.read_csv(train_idx_path, header=None)[0].values\n        valid_idx = pd.read_csv(valid_idx_path, header=None)[0].values\n    else:\n        assert train_size, \"please provide either train_size or none of sizes\"\n        if valid_size is None:\n            valid_size = len(X_train) - train_size\n            assert valid_size > 0\n        if train_size + valid_size > len(X_train):\n            warnings.warn('train_size + valid_size = {} exceeds dataset size: {}.'.format(\n                train_size + valid_size, len(X_train)), Warning)\n\n        shuffled_indices = np.random.permutation(np.arange(len(X_train)))\n        train_idx = shuffled_indices[:train_size]\n        valid_idx = shuffled_indices[train_size: train_size + valid_size]\n\n    return dict(\n        X_train=X_train[train_idx], y_train=y_train[train_idx],\n        X_valid=X_train[valid_idx], y_valid=y_train[valid_idx],\n        X_test=X_test, y_test=y_test,\n    )\n\n\ndef fetch_HIGGS(path, train_size=None, valid_size=None, test_size=5 * 10 ** 5, **kwargs):\n    data_path = os.path.join(path, 'higgs.csv')\n    if not os.path.exists(data_path):\n        os.makedirs(path, exist_ok=True)\n        archive_path = os.path.join(path, 'HIGGS.csv.gz')\n        download('https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz', archive_path)\n        with gzip.open(archive_path, 'rb') as f_in:\n            with open(data_path, 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n    n_features = 29\n    types = {i: (np.float32 if i != 0 else np.int) for i in range(n_features)}\n    data = pd.read_csv(data_path, header=None, dtype=types)\n    data_train, data_test = data.iloc[:-test_size], data.iloc[-test_size:]\n\n    X_train, y_train = data_train.iloc[:, 1:].values, data_train.iloc[:, 0].values\n    X_test, y_test = data_test.iloc[:, 1:].values, data_test.iloc[:, 0].values\n\n    if all(sizes is None for sizes in (train_size, valid_size)):\n        train_idx_path = os.path.join(path, 'stratified_train_idx.txt')\n        valid_idx_path = os.path.join(path, 'stratified_valid_idx.txt')\n        if not all(os.path.exists(fname) for fname in (train_idx_path, valid_idx_path)):\n            download(\"https://www.dropbox.com/s/i2uekmwqnp9r4ix/stratified_train_idx.txt?dl=1\", train_idx_path)\n            download(\"https://www.dropbox.com/s/wkbk74orytmb2su/stratified_valid_idx.txt?dl=1\", valid_idx_path)\n        train_idx = pd.read_csv(train_idx_path, header=None)[0].values\n        valid_idx = pd.read_csv(valid_idx_path, header=None)[0].values\n    else:\n        assert train_size, \"please provide either train_size or none of sizes\"\n        if valid_size is None:\n            valid_size = len(X_train) - train_size\n            assert valid_size > 0\n        if train_size + valid_size > len(X_train):\n            warnings.warn('train_size + valid_size = {} exceeds dataset size: {}.'.format(\n                train_size + valid_size, len(X_train)), Warning)\n\n        shuffled_indices = np.random.permutation(np.arange(len(X_train)))\n        train_idx = shuffled_indices[:train_size]\n        valid_idx = shuffled_indices[train_size: train_size + valid_size]\n\n    return dict(\n        X_train=X_train[train_idx], y_train=y_train[train_idx],\n        X_valid=X_train[valid_idx], y_valid=y_train[valid_idx],\n        X_test=X_test, y_test=y_test,\n    )\n\n\ndef fetch_MICROSOFT(path, **kwargs):\n    train_path = os.path.join(path, 'msrank_train.tsv')\n    test_path = os.path.join(path, 'msrank_test.tsv')\n    if not all(os.path.exists(fname) for fname in (train_path, test_path)):\n        os.makedirs(path, exist_ok=True)\n        download(\"https://www.dropbox.com/s/izpty5feug57kqn/msrank_train.tsv?dl=1\", train_path)\n        download(\"https://www.dropbox.com/s/tlsmm9a6krv0215/msrank_test.tsv?dl=1\", test_path)\n\n        for fname in (train_path, test_path):\n            raw = open(fname).read().replace('\\\\t', '\\t')\n            with open(fname, 'w') as f:\n                f.write(raw)\n\n    data_train = pd.read_csv(train_path, header=None, skiprows=1, sep='\\t')\n    data_test = pd.read_csv(test_path, header=None, skiprows=1, sep='\\t')\n\n    train_idx_path = os.path.join(path, 'train_idx.txt')\n    valid_idx_path = os.path.join(path, 'valid_idx.txt')\n    if not all(os.path.exists(fname) for fname in (train_idx_path, valid_idx_path)):\n        download(\"https://www.dropbox.com/s/pba6dyibyogep46/train_idx.txt?dl=1\", train_idx_path)\n        download(\"https://www.dropbox.com/s/yednqu9edgdd2l1/valid_idx.txt?dl=1\", valid_idx_path)\n    train_idx = pd.read_csv(train_idx_path, header=None)[0].values\n    valid_idx = pd.read_csv(valid_idx_path, header=None)[0].values\n\n    X_train, y_train, query_train = data_train.iloc[train_idx, 2:].values, data_train.iloc[train_idx, 0].values, data_train.iloc[train_idx, 1].values\n    X_valid, y_valid, query_valid = data_train.iloc[valid_idx, 2:].values, data_train.iloc[valid_idx, 0].values, data_train.iloc[valid_idx, 1].values\n    X_test, y_test, query_test = data_test.iloc[:, 2:].values, data_test.iloc[:, 0].values, data_test.iloc[:, 1].values\n\n    return dict(\n        X_train=X_train.astype(np.float32), y_train=y_train.astype(np.int64), query_train=query_train,\n        X_valid=X_valid.astype(np.float32), y_valid=y_valid.astype(np.int64), query_valid=query_valid,\n        X_test=X_test.astype(np.float32), y_test=y_test.astype(np.int64), query_test=query_test,\n    )\n\n\ndef fetch_YAHOO(path, *args):\n    train_path = os.path.join(path, 'yahoo_train.tsv')\n    valid_path = os.path.join(path, 'yahoo_valid.tsv')\n    test_path = os.path.join(path, 'yahoo_test.tsv')\n    if not all(os.path.exists(fname) for fname in (train_path, valid_path, test_path)):\n        os.makedirs(path, exist_ok=True)\n        train_archive_path = os.path.join(path, 'yahoo_train.tsv.gz')\n        valid_archive_path = os.path.join(path, 'yahoo_valid.tsv.gz')\n        test_archive_path = os.path.join(path, 'yahoo_test.tsv.gz')\n        if not all(os.path.exists(fname) for fname in (train_archive_path, valid_archive_path, test_archive_path)):\n            download(\"https://www.dropbox.com/s/7rq3ki5vtxm6gzx/yahoo_set_1_train.gz?dl=1\", train_archive_path)\n            download(\"https://www.dropbox.com/s/3ai8rxm1v0l5sd1/yahoo_set_1_validation.gz?dl=1\", valid_archive_path)\n            download(\"https://www.dropbox.com/s/3d7tdfb1an0b6i4/yahoo_set_1_test.gz?dl=1\", test_archive_path)\n\n        for file_name, archive_name in zip((train_path, valid_path, test_path), (train_archive_path, valid_archive_path, test_archive_path)):\n            with gzip.open(archive_name, 'rb') as f_in:\n                with open(file_name, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n\n        for fname in (train_path, valid_path, test_path):\n            raw = open(fname).read().replace('\\\\t', '\\t')\n            with open(fname, 'w') as f:\n                f.write(raw)\n\n    data_train = pd.read_csv(train_path, header=None, skiprows=1, sep='\\t')\n    data_valid = pd.read_csv(valid_path, header=None, skiprows=1, sep='\\t')\n    data_test = pd.read_csv(test_path, header=None, skiprows=1, sep='\\t')\n\n    X_train, y_train, query_train = data_train.iloc[:, 2:].values, data_train.iloc[:, 0].values, data_train.iloc[:, 1].values\n    X_valid, y_valid, query_valid = data_valid.iloc[:, 2:].values, data_valid.iloc[:, 0].values, data_valid.iloc[:, 1].values\n    X_test, y_test, query_test = data_test.iloc[:, 2:].values, data_test.iloc[:, 0].values, data_test.iloc[:, 1].values\n\n    return dict(\n        X_train=X_train.astype(np.float32), y_train=y_train, query_train=query_train,\n        X_valid=X_valid.astype(np.float32), y_valid=y_valid, query_valid=query_valid,\n        X_test=X_test.astype(np.float32), y_test=y_test, query_test=query_test,\n    )\n\n\ndef fetch_CLICK(path, valid_size=100_000, seed=None, **kwargs):\n    # based on: https://www.kaggle.com/slamnz/primer-airlines-delay\n    csv_path = os.path.join(path, 'click.csv')\n    if not os.path.exists(csv_path):\n        os.makedirs(path, exist_ok=True)\n        download('https://www.dropbox.com/s/w43ylgrl331svqc/click.csv?dl=1', csv_path)\n\n    data = pd.read_csv(csv_path, index_col=0)\n    X, y = data.drop(columns=['target']), data['target']\n    X_train, X_test = X[:-100_000].copy(), X[-100_000:].copy()\n    y_train, y_test = y[:-100_000].copy(), y[-100_000:].copy()\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n\n    cat_features = ['url_hash', 'ad_id', 'advertiser_id', 'query_id',\n                    'keyword_id', 'title_id', 'description_id', 'user_id']\n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train, y_train, test_size=valid_size, random_state=seed)\n\n    cat_encoder = LeaveOneOutEncoder()\n    cat_encoder.fit(X_train[cat_features], y_train)\n    X_train[cat_features] = cat_encoder.transform(X_train[cat_features])\n    X_val[cat_features] = cat_encoder.transform(X_val[cat_features])\n    X_test[cat_features] = cat_encoder.transform(X_test[cat_features])\n    return dict(\n        X_train=X_train.values.astype('float32'), y_train=y_train,\n        X_valid=X_val.values.astype('float32'), y_valid=y_val,\n        X_test=X_test.values.astype('float32'), y_test=y_test\n    )\n\ndef fetch_MUSHROOMS(path, valid_size=0.2, test_size=0.2, seed=None):\n\n    path = Path(path)\n    data_path = path / 'agaricus-lepiota.data'\n\n    if not data_path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n\n        download('https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data', data_path)\n\n    data = pd.read_csv(data_path, names=np.arange(23))\n    encoder = OrdinalEncoder(return_df=False)\n    data = encoder.fit_transform(data)\n    \n    X, Y = (data[:, 1:]).astype(np.float32), (data[:, 0] - 1).astype(int)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=seed)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=valid_size / (1 - test_size), random_state=seed)\n\n    return dict(\n        X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test\n    )\n\ndef fetch_TICTACTOE(path, valid_size=0.2, test_size=0.2, seed=None):\n\n    path = Path(path)\n    data_path = path / 'tic-tac-toe.data'\n\n    if not data_path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n\n        download('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data', data_path)\n\n    data = pd.read_csv(data_path, names=np.arange(10))\n    encoder = OrdinalEncoder(return_df=False)\n    data = encoder.fit_transform(data)\n    \n    X, Y = (data[:, :-1]).astype(np.float32), (data[:, -1] - 1).astype(int)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=seed)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=valid_size / (1 - test_size), random_state=seed)\n\n    return dict(\n        X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test\n    )\n\n#**from src.toy_datasets import ***\n\n# ------------------------------------------------------------ TOY DATASETS\n\ndef toy_dataset(n=1000, distr=\"xor\", dim=2):\n\n    if distr == \"xor\":\n        \n        X = np.random.uniform(low=tuple([-1.] * dim), high=tuple([1.] * dim), size=(n, dim))\n        Y = (X[:,0] * X[:,1] >= 0).astype(int)\n\n        return dict(X=X.astype(np.float32), Y=Y)\n\n    elif distr == \"reg-xor\":\n        \n        X = np.random.uniform(low=tuple([-1.] * dim), high=tuple([1.] * dim), size=(n, dim))\n        labels = (X[:,0] * X[:,1] >= 0)\n\n        Y = np.empty(n)\n        Y[labels] = np.random.normal(0.8, 0.1, np.sum(labels))\n        Y[~labels] = np.random.normal(0.2, 0.1, np.sum(~labels))\n\n        return dict(X=X.astype(np.float32), Y=Y.astype(np.float32), labels=labels)\n\n    elif distr == \"swissroll\":\n\n        n2 = n // 2\n\n        X1,_ = make_swiss_roll(n_samples=n2, noise=0)\n        Y1 = np.ones(n2)\n\n        X2 = np.random.uniform(low=tuple([-1.] * dim), high=tuple([1.] * dim), size=(n2, dim))\n        Y2 = np.zeros(n2)\n\n        X = np.r_[X1[:,::2] / 15, X2]\n        Y = np.r_[Y1, Y2]\n\n        return dict(X=X.astype(np.float32), Y=Y)\n\n    else:\n        NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.651612Z","iopub.execute_input":"2023-09-28T20:47:42.652503Z","iopub.status.idle":"2023-09-28T20:47:42.768778Z","shell.execute_reply.started":"2023-09-28T20:47:42.652467Z","shell.execute_reply":"2023-09-28T20:47:42.767926Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Imports**","metadata":{}},{"cell_type":"markdown","source":"**Preprocess Data**","metadata":{}},{"cell_type":"code","source":"def preprocess_data_adult(data_path):\n    # Read the data into a DataFrame\n    columns = [\n        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n        \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n        \"hours-per-week\", \"native-country\", \"income\"\n    ]\n    df = pd.read_csv(data_path, names=columns, na_values=[\" ?\"])\n\n    # Drop rows with missing values\n    df.dropna(inplace=True)\n\n    # Convert categorical features using Label Encoding\n    categorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n    label_encoders = {}\n    for col in categorical_columns:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n        label_encoders[col] = le\n\n    # Encode the target variable\n    df[\"income\"] = df[\"income\"].apply(lambda x: 1 if x == \" >50K\" else 0)\n\n    return df\n\ndef preprocess_data_bank_marketing(data):\n    # Convert categorical features using Label Encoding\n    label_encoders = {}\n    for col in data.select_dtypes(include=['object']).columns:\n        le = LabelEncoder()\n        data[col] = le.fit_transform(data[col])\n        label_encoders[col] = le\n\n    return data\n\ndef preprocess_data_credit_card_defaults(data):\n    # Convert categorical features using one-hot encoding\n    data = pd.get_dummies(data, columns=[\"SEX\", \"EDUCATION\", \"MARRIAGE\"], drop_first=True)\n\n    # Standardize numerical features\n    scaler = StandardScaler()\n    data[[\"LIMIT_BAL\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \"BILL_AMT1\",\n          \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\",\n          \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]] = scaler.fit_transform(\n        data[[\"LIMIT_BAL\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \"BILL_AMT1\",\n               \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\",\n               \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]])\n\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.770833Z","iopub.execute_input":"2023-09-28T20:47:42.771585Z","iopub.status.idle":"2023-09-28T20:47:42.787308Z","shell.execute_reply.started":"2023-09-28T20:47:42.771551Z","shell.execute_reply":"2023-09-28T20:47:42.786285Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Fetch Data**","metadata":{}},{"cell_type":"code","source":"def fetch_ADULT(data_dir=\"./ADULT_DATA\"):\n    print(\"---------------------ADULT--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n        \n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/2/adult.zip\"\n    zip_file_path = os.path.join(data_dir, \"adult.zip\")\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n\n    # Preprocess the data\n    train_data_path = os.path.join(data_dir, \"adult.data\")\n#     test_data_path = os.path.join(data_dir, \"adult.test\")\n    df_train = preprocess_data_adult(train_data_path)\n#     df_test = preprocess_data_adult(test_data_path)\n\n    # Split the data into train, validation, and test sets\n    X = df_train.drop(\"income\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df_train[\"income\"]\n    \n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n#     X_test = df_test.drop(\"income\", axis=1)\n#     y_test = df_test[\"income\"]\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents using shutil.rmtree()\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train, X_valid=X_val.astype('float32'), y_valid=y_val, X_test=X_test.astype('float32'), y_test=y_test\n    )\n\ndef fetch_bank_marketing(data_dir=\"./BANK\"):\n    print(\"---------------------BANK--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\"\n    zip_file_path = os.path.join(data_dir, \"bank_marketing.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n    \n    zip_file_path_bank_add = os.path.join(data_dir, \"bank-additional.zip\")\n    with zipfile.ZipFile(zip_file_path_bank_add, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n\n    # Get the extracted directory path\n    extracted_dir = os.path.join(data_dir, \"bank-additional\")\n\n    # Read the dataset\n    data = pd.read_csv(os.path.join(extracted_dir, \"bank-additional-full.csv\"), sep=';')\n\n    # Preprocess the data\n    data = preprocess_data_bank_marketing(data)\n\n    # Split the data into train, validation, and test sets\n    X = data.drop(\"y\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"y\"]\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,X_test=X_test.astype('float32'), y_test=y_test, X_valid = X_val.astype('float32'), y_valid = y_val\n    )\n\ndef fetch_credit_card_defaults(data_dir=\"./CREDIT\"):\n    print(\"---------------------CREDIT--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    !pip install xlrd\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip\"\n    zip_file_path = os.path.join(data_dir, \"credit_card_defaults.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n\n#     # Get the extracted directory path\n#     extracted_dir = os.path.join(data_dir, \"default+of+credit+card+clients\")\n\n    # Read the dataset\n    data = pd.read_excel(os.path.join(data_dir, \"default of credit card clients.xls\"), skiprows=1)\n\n    # Preprocess the data\n    data = preprocess_data_credit_card_defaults(data)\n\n    # Split the data into train, validation, and test sets\n    X = data.drop(\"default payment next month\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"default payment next month\"]\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train, X_valid=X_val.astype('float32'), y_valid=y_val , X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_gamma_telescope(data_dir=\"./TELESCOPE\"):\n    print(\"---------------------TELESCOPE--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/159/magic+gamma+telescope.zip\"\n    zip_file_path = os.path.join(data_dir, \"magic_gamma_telescope.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n    \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"magic04.data\")\n    columns = [\n        \"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\", \"fM3Long\",\n        \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"\n    ]\n    data = pd.read_csv(data_path, header=None, names=columns)\n    \n    # Convert the class labels to binary format (g = gamma, h = hadron)\n    data[\"class\"] = data[\"class\"].map({\"g\": 1, \"h\": 0})\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_rice_dataset(data_dir=\"./RICE\"):\n    print(\"---------------------RICE--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/545/rice+cammeo+and+osmancik.zip\"\n    zip_file_path = os.path.join(data_dir, \"rice_dataset.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    arff_file_name = os.path.join(data_dir, \"Rice_Cammeo_Osmancik.arff\")\n\n    \n    # Load the ARFF file using SciPy\n    data, meta = arff.loadarff(arff_file_name)\n    \n    df = pd.DataFrame(data)\n    df[\"Class\"] = df[\"Class\"].map({b'Cammeo': 1, b'Osmancik': 0})\n    \n    # Split the data into features (X) and target (y)\n    X = df.drop(\"Class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[\"Class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_german_credit_data(data_dir=\"./GERMAN\"):\n    print(\"---------------------GERMAN--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"http://archive.ics.uci.edu/static/public/144/statlog+german+credit+data.zip\"\n    zip_file_path = os.path.join(data_dir, \"german_credit_data.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"german.data\")\n\n    columns = [\n        \"checking_account_status\", \"duration_months\", \"credit_history\", \"purpose\",\n        \"credit_amount\", \"savings_account_bonds\", \"employment\", \"installment_rate\",\n        \"personal_status_sex\", \"other_debtors_guarantors\", \"present_residence\",\n        \"property\", \"age\", \"other_installment_plans\", \"housing\", \"existing_credits\",\n        \"job\", \"num_dependents\", \"own_telephone\", \"foreign_worker\", \"class\"\n    ]\n    data = pd.read_csv(data_path, sep=' ', header=None, names=columns)\n    \n    # Convert the class labels to binary format (1 = Good, 2 = Bad)\n    data[\"class\"] = data[\"class\"].map({1: 1, 2: 0})\n    \n    # Handle null values (replace with appropriate values)\n    data.fillna(method='ffill', inplace=True)  # Forward fill\n    \n    # Convert categorical variables to dummy variables\n    categorical_columns = [\n        \"checking_account_status\", \"credit_history\", \"purpose\", \"savings_account_bonds\",\n        \"employment\", \"personal_status_sex\", \"other_debtors_guarantors\", \"property\",\n        \"other_installment_plans\", \"housing\", \"job\", \"own_telephone\", \"foreign_worker\"\n    ]\n    data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_spambase_dataset(data_dir=\"./SPAM\"):\n    print(\"---------------------SPAM--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"http://archive.ics.uci.edu/static/public/94/spambase.zip\"\n    zip_file_path = os.path.join(data_dir, \"spambase.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"spambase.data\")\n\n    columns = [\n        f\"f{i}\" for i in range(57)\n    ] + [\"spam\"]\n    data = pd.read_csv(data_path, header=None, names=columns)\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"spam\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"spam\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_accelerometer_gyro_dataset(data_dir=\"./GYRO\"):\n    print(\"---------------------GYRO--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/755/accelerometer+gyro+mobile+phone+dataset.zip\"\n    zip_file_path = os.path.join(data_dir, \"accelerometer_gyro_dataset.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"accelerometer_gyro_mobile_phone_dataset.csv\")\n    \n    data = pd.read_csv(data_path)\n    \n    # Convert categorical column to numeric (e.g., label encoding)\n    data[\"timestamp\"] = data[\"timestamp\"].astype(\"category\").cat.codes\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"Activity\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"Activity\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_swarm_behaviour(data_dir=\"./SWARM\"):\n    print(\"---------------------SWARM--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/524/swarm+behaviour.zip\"\n    zip_file_path = os.path.join(data_dir, \"swarm_behaviour.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"Swarm Behavior Data/Grouped.csv\")\n    \n    data = pd.read_csv(data_path)\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"Class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"Class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir) \n    return data_splits","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.790387Z","iopub.execute_input":"2023-09-28T20:47:42.790718Z","iopub.status.idle":"2023-09-28T20:47:42.897179Z","shell.execute_reply.started":"2023-09-28T20:47:42.790687Z","shell.execute_reply":"2023-09-28T20:47:42.896283Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#**class Dataset:**\n\nREAL_DATASETS = {\n    'A9A': fetch_A9A,\n    'EPSILON': fetch_EPSILON,\n    'PROTEIN': fetch_PROTEIN,\n    'YEAR': fetch_YEAR,\n    'MICROSOFT': fetch_MICROSOFT,\n    'YAHOO': fetch_YAHOO,\n    'CLICK': fetch_CLICK,\n    'GLASS': fetch_GLASS,\n    'COVTYPE': fetch_COVTYPE,\n    'ALOI': fetch_ALOI,\n    'DIGITS': fetch_DIGITS,\n    'MUSH': fetch_MUSHROOMS,\n    'TTT': fetch_TICTACTOE,\n    ####### 10 latest UCI datasets ########\n    'ADULT': fetch_ADULT,\n    'bank_marketing': fetch_bank_marketing,\n    'credit_card_defaults': fetch_credit_card_defaults,\n    'gamma_telescope': fetch_gamma_telescope,\n    'rice_dataset': fetch_rice_dataset,\n    'german_credit_data': fetch_german_credit_data,\n    'spambase_dataset': fetch_spambase_dataset,\n    'accelerometer_gyro_dataset': fetch_accelerometer_gyro_dataset,\n    'swarm_behaviour': fetch_swarm_behaviour,\n    'HIGGS': fetch_HIGGS,\n}\n\nTOY_DATASETS = [\n    'xor',\n    'reg-xor',\n    'swissroll',\n]\n\nclass Dataset:\n    def __init__(self, dataset, data_path='./DATA', normalize=False, normalize_target=False, quantile_transform=False, quantile_noise=1e-3, in_features=None, out_features=None, flatten=False, **kwargs):\n        \"\"\"\n        Dataset is a dataclass that contains all training and evaluation data required for an experiment\n        :param dataset: a pre-defined dataset name (see DATASETS) or a custom dataset\n            Your dataset should be at (or will be downloaded into) {data_path}/{dataset}\n        :param data_path: a shared data folder path where the dataset is stored (or will be downloaded into)\n        :param normalize: standardize features by removing the mean and scaling to unit variance\n        :param quantile_transform: whether tranform the feature distributions into normals, using a quantile transform\n        :param quantile_noise: magnitude of the quantile noise\n        :param in_features: which features to use as inputs\n        :param out_features: which features to reconstruct as output\n        :param flatten: whether flattening instances to vectors\n        :param kwargs: depending on the dataset, you may select train size, test size or other params\n        \"\"\"\n\n        if dataset in REAL_DATASETS:\n            data_dict = REAL_DATASETS[dataset](Path(data_path) / dataset, **kwargs)\n\n            self.X_train = data_dict['X_train']\n            self.y_train = data_dict['y_train']\n            self.X_valid = data_dict['X_valid']\n            self.y_valid = data_dict['y_valid']\n            self.X_test = data_dict['X_test']\n            self.y_test = data_dict['y_test']\n\n            if flatten:\n                self.X_train, self.X_valid, self.X_test = self.X_train.reshape(len(self.X_train), -1), self.X_valid.reshape(len(self.X_valid), -1), self.X_test.reshape(len(self.X_test), -1)\n\n            if normalize:\n\n                print(\"Normalize dataset\")\n                axis = [0] + [i + 2 for i in range(self.X_train.ndim - 2)]\n                self.mean = np.mean(self.X_train, axis=tuple(axis), dtype=np.float32)\n                self.std = np.std(self.X_train, axis=tuple(axis), dtype=np.float32)\n\n                # if constants, set std to 1\n                self.std[self.std == 0.] = 1.\n\n                if dataset not in ['ALOI']:\n                    self.X_train = (self.X_train - self.mean) / self.std\n                    self.X_valid = (self.X_valid - self.mean) / self.std\n                    self.X_test = (self.X_test - self.mean) / self.std\n\n            if quantile_transform:\n                quantile_train = np.copy(self.X_train)\n                if quantile_noise:\n                    stds = np.std(quantile_train, axis=0, keepdims=True)\n                    noise_std = quantile_noise / np.maximum(stds, quantile_noise)\n                    quantile_train += noise_std * np.random.randn(*quantile_train.shape)\n\n                qt = QuantileTransformer(output_distribution='normal').fit(quantile_train)\n                self.X_train = qt.transform(self.X_train)\n                self.X_valid = qt.transform(self.X_valid)\n                self.X_test = qt.transform(self.X_test)\n\n            if normalize_target:\n\n                print(\"Normalize target value\")\n                self.mean_y = np.mean(self.y_train, axis=0, dtype=np.float32)\n                self.std_y = np.std(self.y_train, axis=0, dtype=np.float32)\n\n                # if constants, set std to 1\n                if self.std_y == 0.:\n                    self.std_y = 1.\n\n                self.y_train = (self.y_train - self.mean_y) / self.std_y\n                self.y_valid = (self.y_valid - self.mean_y) / self.std_y\n                self.y_test = (self.y_test - self.mean_y) / self.std_y\n\n            if in_features is not None:\n                self.X_train_in, self.X_valid_in, self.X_test_in = self.X_train[:, in_features], self.X_valid[:, in_features], self.X_test[:, in_features]\n\n            if out_features is not None:\n                self.X_train_out, self.X_valid_out, self.X_test_out = self.X_train[:, out_features], self.X_valid[:, out_features], self.X_test[:, out_features]\n\n        elif dataset in TOY_DATASETS:\n            data_dict = toy_dataset(distr=dataset, **kwargs)\n\n            self.X = data_dict['X']\n            self.Y = data_dict['Y']\n            if 'labels' in data_dict:\n                self.labels = data_dict['labels']\n\n        self.data_path = data_path\n        self.dataset = dataset\n\nclass TorchDataset(torch.utils.data.Dataset):\n\n    def __init__(self, *data, **options):\n        \n        n_data = len(data)\n        if n_data == 0:\n            raise ValueError(\"At least one set required as input\")\n\n        self.data = data\n        means = options.pop('means', None)\n        stds = options.pop('stds', None)\n        self.transform = options.pop('transform', None)\n        self.test = options.pop('test', False)\n        \n        if options:\n            raise TypeError(\"Invalid parameters passed: %s\" % str(options))\n         \n        if means is not None:\n            assert stds is not None, \"must specify both <means> and <stds>\"\n\n            self.normalize = lambda data: [(d - m) / s for d, m, s in zip(data, means, stds)]\n\n        else:\n            self.normalize = lambda data: data\n\n    def __len__(self):\n        return len(self.data[0])\n\n    def __getitem__(self, idx):\n        data = self.normalize([s[idx] for s in self.data])\n        if self.transform:\n\n            if self.test:\n                data = sum([[self.transform.test_transform(d)] * 2 for d in data], [])\n            else:\n                data = sum([self.transform(d) for d in data], [])\n            \n        return data","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.898829Z","iopub.execute_input":"2023-09-28T20:47:42.899408Z","iopub.status.idle":"2023-09-28T20:47:42.929593Z","shell.execute_reply.started":"2023-09-28T20:47:42.899376Z","shell.execute_reply":"2023-09-28T20:47:42.928636Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**DLGN Model**","metadata":{}},{"cell_type":"code","source":"def set_npseed(seed):\n    np.random.seed(seed)\n\n\ndef set_torchseed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.931863Z","iopub.execute_input":"2023-09-28T20:47:42.933040Z","iopub.status.idle":"2023-09-28T20:47:42.946704Z","shell.execute_reply.started":"2023-09-28T20:47:42.933008Z","shell.execute_reply":"2023-09-28T20:47:42.945792Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DLGN_FC(nn.Module):\n    def __init__(self, to_copy=None, input_dim=None, output_dim=None, num_hidden_nodes=[],\n                NPF_pretrained=True,NPV_pretrained=True, beta=30, mode='pwc'):\n        \n        # if to_copy is not none \n            # NPF_pretrained = True means copy the to_copy networks gating model\n            # NPV_pretrained = True means copy the to_copy networks value model\n        super(DLGN_FC, self).__init__()\n        if to_copy==None:\n            self.num_hidden_layers = len(num_hidden_nodes)\n            self.beta=beta  # Soft gating parameter\n            self.mode = mode\n            self.num_nodes=[input_dim]+num_hidden_nodes+[output_dim]\n            self.gating_layers=[]\n            self.value_layers=[]\n            self.gating_model = nn.Sequential(*self.gating_layers)\n            for i in range(self.num_hidden_layers+1):\n                if i!=self.num_hidden_layers:\n                    self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=True).to(device))\n                self.value_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False).to(device))\n            self.gating_model = nn.Sequential(*self.gating_layers)\n            self.value_model = nn.Sequential(*self.value_layers)\n        else:\n            self.gating_layers=[]\n            self.value_layers=[]\n            self.num_hidden_layers = to_copy.num_hidden_layers\n            self.beta=to_copy.beta  # Soft gating parameter\n            self.mode = to_copy.mode\n            self.num_nodes=list(to_copy.num_nodes)\n            for i in range(self.num_hidden_layers+1):\n                if i!=self.num_hidden_layers:\n                    self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=True).to(device))\n                self.value_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False).to(device))\n                if NPF_pretrained:\n                    if i!=self.num_hidden_layers:\n                        self.gating_layers[i].weight.data =  torch.Tensor(\n                            np.array(to_copy.gating_layers[i].cpu().weight.detach().numpy()))\n                        self.gating_layers[i].bias.data = torch.Tensor(\n                            np.array(to_copy.gating_layers[i].cpu().bias.detach().numpy()))\n                if NPV_pretrained:\n                    self.value_layers[i].weight.data = torch.Tensor(\n                        np.array(to_copy.value_layers[i].cpu().weight.detach().numpy()))\n            self.gating_model = nn.Sequential(*self.gating_layers)\n            self.value_model = nn.Sequential(*self.value_layers)\n            \n    def return_gating_functions(self):\n        effective_weights = []\n        effective_biases =[]\n        for i in range(self.num_hidden_layers):\n            curr_weight = self.gating_layers[i].weight.detach()\n            curr_bias = self.gating_layers[i].bias.detach()\n            if i==0:\n                effective_weights.append(curr_weight)\n                effective_biases.append(curr_bias)\n            else:\n                effective_biases.append(torch.matmul(curr_weight,effective_biases[-1])+curr_bias)\n                effective_weights.append(torch.matmul(curr_weight,effective_weights[-1]))\n        return effective_weights, effective_biases\n        # effective_weights (and effective biases) is a list of size num_hidden_layers\n\n    def forward(self, x):\n        gate_scores=[x.to(device)]\n        if self.mode=='pwc':\n            values=[torch.ones(x.shape).to(device)]\n        else:\n            values=[x.to(device)]\n\n        for i in range(self.num_hidden_layers):\n            gate_scores.append(self.gating_layers[i].to(device)(gate_scores[-1].to(device)))\n            curr_gate_on_off = torch.sigmoid(self.beta * gate_scores[-1])\n            values.append(self.value_layers[i].to(device)(values[-1].to(device))*curr_gate_on_off.to(device))\n        values.append(self.value_layers[self.num_hidden_layers].to(device)(values[-1].to(device)))\n        # Values is a list of size 1+num_hidden_layers+1\n        #gate_scores is a list of size 1+num_hidden_layers\n        return values,gate_scores","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.949035Z","iopub.execute_input":"2023-09-28T20:47:42.950231Z","iopub.status.idle":"2023-09-28T20:47:42.972767Z","shell.execute_reply.started":"2023-09-28T20:47:42.950206Z","shell.execute_reply":"2023-09-28T20:47:42.971771Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#from src.optimization import train_stochastic, evaluate\n\ndef train_stochastic(dataloader, model, optimizer, criterion,criterion2, epoch,parameter_mask=dict(), monitor=None, prog_bar=True):\n\n    model.train()\n\n    last_iter = epoch * len(dataloader)\n#     print(\"last_iter inside train_stochastic:\",last_iter)\n    train_obj = 0.\n    total_losses=0.\n    if prog_bar:\n        pbar = tqdm(dataloader)\n    else:\n        pbar = dataloader\n\n    for i, batch in enumerate(pbar):\n        optimizer.zero_grad()        \n\n        t_x, t_y = batch\n        t_x = t_x.to(device)\n        t_y = t_y.to(device)\n        if(len(t_x)>1):\n            if t_y.dim() > 2: # predictors support only flatten output atm\n                t_y = t_y.view(len(t_y), -1).to(device)\n\n            values,gate_scores = model(t_x)\n            y_pred = torch.sigmoid(values[-1])\n            y_pred = y_pred.squeeze()\n            loss = criterion(y_pred, t_y) / len(t_x)\n            train_obj += loss.cpu().detach().numpy()\n\n            ##Testing 0-1 loss\n            y_pred1=y_pred.clone()\n            y_pred1[y_pred > 0.5] = 1\n            y_pred1[y_pred <= 0.5] = 0\n            y_pred1 = y_pred1.detach()\n            loss2 = criterion2(y_pred1, t_y)/ len(t_x)\n            total_losses += loss2.cpu().detach()\n            ##Testing 0-1 loss\n\n\n            if prog_bar:\n                pbar.set_description(\"avg train loss %f\" % (total_losses / (i + 1)))\n            loss.backward()\n            for name,param in model.named_parameters():\n                param.grad *= parameter_mask[name]\n\n            optimizer.step()\n\n            if monitor:\n                monitor.write(model, i + last_iter, report_tree=True, train={\"Loss\": loss.cpu().detach()})\n            \ndef evaluate(dataloader, model, criteria, epoch=None, monitor=None):\n\n    model.eval()\n    total_losses = {k: 0. for k in criteria.keys()}\n    num_points = 0\n    for batch in dataloader:\n\n        t_x, t_y = batch\n        t_x = t_x.to(device)\n        t_y = t_y.to(device)\n        \n        if t_y.dim() > 2: # predictors support only flatten output atm\n            t_y = t_y.view(len(t_y), -1)\n        \n        num_points += len(t_x)\n        \n        values,gate_scores = model(t_x)\n        \n        y_pred = torch.sigmoid(values[-1])\n        y_pred = y_pred.squeeze()\n\n        y_pred[y_pred > 0.5] = 1\n        y_pred[y_pred <= 0.5] = 0\n        y_pred = y_pred.detach()\n\n        for k in criteria.keys():\n            loss = criteria[k](y_pred, t_y)\n            total_losses[k] += loss.cpu().detach()\n\n    if monitor:\n        monitor.write(model, epoch, val={k: loss / num_points for k, loss in total_losses.items()})\n\n    return {k: loss.cpu().numpy() / num_points for k, loss in total_losses.items()}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.974930Z","iopub.execute_input":"2023-09-28T20:47:42.975623Z","iopub.status.idle":"2023-09-28T20:47:42.993300Z","shell.execute_reply.started":"2023-09-28T20:47:42.975592Z","shell.execute_reply":"2023-09-28T20:47:42.992375Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train_dlgn_classification (DLGN_obj,trainloader,valloader,num_epoch=1,parameter_mask=dict()):\n    # DLGN_obj is the initial network\n    # parameter_mask is a dictionary compatible with dict(DLGN_obj.named_parameters())\n    # if a key corresponding to a named_parameter is not present it is assumed to be all ones (i.e it will be updated)\n    \n    set_torchseed(seed)\n    DLGN_obj.to(device)\n    DLGN_obj_initial = DLGN_FC(to_copy=DLGN_obj)\n    DLGN_obj_return = DLGN_FC(to_copy=DLGN_obj)\n    \n    \n    \n    if model_type == \"Classification\":\n        # init loss\n        loss = nn.BCELoss(reduction=\"sum\")\n        criterion = lambda x, y: loss(x.float(), y.float())\n\n        # evaluation criterion => error rate\n        eval_criterion = lambda x, y: (x != y).sum()\n       \n    if model_type == \"Regression\":\n        criterion = nn.MSELoss()\n\n\n    DLGN_params = []\n    DLGN_params += [item.weight for item in DLGN_obj.gating_layers]\n    DLGN_params += [item.bias for item in DLGN_obj.gating_layers]\n    DLGN_params += [item.weight for item in DLGN_obj.value_layers]\n\n    \n    if optimizer_name == 'SGD':\n        optimizer = optim.SGD(DLGN_obj.parameters(), lr=lr)\n    if optimizer_name == 'Adam':\n        optimizer = optim.Adam(DLGN_obj.parameters(), lr=lr)\n        \n        \n#     # init optimizer\n#     optimizer = QHAdam(model.parameters(), lr=LR, nus=(0.7, 1.0), betas=(0.995, 0.998))\n    \n    \n    losses=[]\n    DLGN_obj_store = []\n    test_losses, train_times, test_times = [], [], []\n    \n\n    best_val_loss = float(\"inf\")\n    best_e = -1\n    no_improv = 0\n    t0 = time.time()\n    for e in range(num_epoch):\n        train_stochastic(trainloader, DLGN_obj, optimizer, criterion,eval_criterion, epoch=e,parameter_mask=parameter_mask, monitor=None,prog_bar=False)\n\n        val_loss = evaluate(valloader, DLGN_obj, {'ER': eval_criterion}, epoch=e, monitor=None)\n#         print(\"Epoch %i: validation loss = %f\\n\" % (e, val_loss[\"ER\"]))\n        no_improv += 1\n        if val_loss[\"ER\"] < best_val_loss:\n            best_val_loss = val_loss[\"ER\"]\n            best_e = e\n            no_improv = 0\n#             if perfectSq(epoch):\n            DLGN_obj_store.append(DLGN_FC(to_copy=DLGN_obj))\n            DLGN_obj_return = DLGN_FC(to_copy=DLGN_obj)\n#             LTBinaryClassifier.save_model(model, optimizer, state, save_dir, epoch=e, val_er=best_val_loss)\n\n        if no_improv == num_epoch // 5:\n            break\n    t1 = time.time()\n\n    print(\"best validation error rate (epoch {}): {}\\n\".format(best_e, best_val_loss))\n\n    \n    model = DLGN_obj_return\n    t2 = time.time()\n    test_loss = evaluate(testloader, model, {'ER': eval_criterion})\n    print(\"test error rate (model of epoch {}): {}\\n\".format(best_e, test_loss['ER']))\n    print(\"test accuracy (model of epoch {}): {}\\n\".format(best_e,1-test_loss['ER']))\n\n#     t3 = time.time()\n#     test_losses.append(test_loss['ER'])\n#     train_times.append(t1 - t0)\n#     test_times.append(t3 - t2)\n\n#     print(np.mean(test_losses), np.std(test_losses))\n# #     np.save(save_dir / '../test-losses.npy', test_losses)\n#     print(\"Avg train time\", np.mean(train_times))\n#     print(\"Avg test time\", np.mean(test_times))\n    \n    return DLGN_obj_return, DLGN_obj_store","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:42.996348Z","iopub.execute_input":"2023-09-28T20:47:42.996602Z","iopub.status.idle":"2023-09-28T20:47:43.013840Z","shell.execute_reply.started":"2023-09-28T20:47:42.996580Z","shell.execute_reply":"2023-09-28T20:47:43.012861Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model_func(DLGN_init=None,trainloader=None,valloader=None,num_epoch=0,NPF_freeze=False,NPV_freeze=False):\n    parameter_mask=dict()\n    for name,parameter in DLGN_init.named_parameters():\n        if name[:5]==\"value_\"[:5]:\n            if NPV_freeze:\n                parameter_mask[name]=torch.zeros_like(parameter) # freezing all value network layers\n            else:\n                parameter_mask[name]=torch.ones_like(parameter) # Updating all value network layers\n        if name[:5]==\"gating_\"[:5]:\n            if NPF_freeze:\n                parameter_mask[name]=torch.zeros_like(parameter) # freezing all NPF layers\n            else:\n                parameter_mask[name]=torch.ones_like(parameter) # Updating all NPF layers\n                \n    DLGN_obj_return,DLGN_obj_store = train_dlgn_classification(DLGN_obj=DLGN_init,trainloader=trainloader,valloader=valloader,num_epoch=num_epoch,parameter_mask=parameter_mask)\n\n    return DLGN_obj_return,DLGN_obj_store","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:43.018866Z","iopub.execute_input":"2023-09-28T20:47:43.019233Z","iopub.status.idle":"2023-09-28T20:47:43.031213Z","shell.execute_reply.started":"2023-09-28T20:47:43.019209Z","shell.execute_reply":"2023-09-28T20:47:43.028591Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"weight_decay=0.0\nnum_hidden_layers=3\n\nmodel_type = \"Classification\"\noutput_dim=1 #num_of_classes\n# num_hidden_nodes=[50,50,50]\nmodep='pwc'\nmodel_name = 'DLGN' #DLGN/DeepnonLinearModel\nbeta = 3\noptimizer_name ='Adam'\n\nseed = 365\nlr = 0.001\nBATCH_SIZE = 512 ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:43.034285Z","iopub.execute_input":"2023-09-28T20:47:43.034697Z","iopub.status.idle":"2023-09-28T20:47:43.045013Z","shell.execute_reply.started":"2023-09-28T20:47:43.034673Z","shell.execute_reply":"2023-09-28T20:47:43.044159Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"num_hidden_nodes_list=[[125,125,125]]#,[5,5,5],[10,10,10],[20,20,20],[75,75,75],[125,125,125],[150,150,150],[200,200,200],[1000,1000,1000]]\nnum_epoch = 256\nDATA_NAME=[\"ADULT\"]#,\"bank_marketing\",\"credit_card_defaults\",\"gamma_telescope\",\"rice_dataset\",\"german_credit_data\",\"spambase_dataset\",\"accelerometer_gyro_dataset\",\"swarm_behaviour\"]#,\"HIGGS\"]\n\nfor data_name in DATA_NAME:\n    for num_hidden_nodes in num_hidden_nodes_list:\n        print(\"=================\",num_hidden_nodes,\"=========================\")\n        data = Dataset(data_name,normalize=True)\n        print('classes', np.unique(data.y_test))\n\n        set_npseed(seed)\n        set_torchseed(seed)\n\n        save_dir = Path(\"./results/tabular-quantile/\") / data_name / \"seed={}\".format(seed)\n        save_dir.mkdir(parents=True, exist_ok=True)\n\n        input_dim=data.X_train.shape[1]\n        trainloader = torch.utils.data.DataLoader(TorchDataset(data.X_train, data.y_train), batch_size=BATCH_SIZE, shuffle=True)\n        valloader = torch.utils.data.DataLoader(TorchDataset(data.X_valid, data.y_valid), batch_size=BATCH_SIZE, shuffle=False)\n        testloader = torch.utils.data.DataLoader(TorchDataset(data.X_test, data.y_test), batch_size=BATCH_SIZE, shuffle=False)\n\n\n        NPF_freeze = False #Default False, set True - when freeze the whole NPF\n        NPV_freeze = False #Default False, set True - when freeze the whole NPV\n\n\n        ## For single run\n        DLGN_init= DLGN_FC(input_dim=input_dim, output_dim=1, num_hidden_nodes=num_hidden_nodes,beta=beta)\n        DLGN_obj_return,DLGN_obj_store=train_model_func(DLGN_init,trainloader,valloader,num_epoch,NPF_freeze,NPV_freeze)\n\n    '''\n\n    ##For multiple runs\n\n    lr_store = [0.001,0.0001,0.00001]\n    num_hidden_nodes_store=[[2,2,2],[5,5,5],[10,10,10],[20,20,20],[50,50,50],[100,100,100],[200,200,200],[500,500,500]]\n\n    for lr in lr_store:\n        for num_hidden_nodes in num_hidden_nodes_store:\n            print(\"=========================================================================================\")\n            print(\"LR Valeu:\",lr)\n            print(\"num_hidden_nodes:\",num_hidden_nodes)\n            print(\"=========================================================================================\")\n            lr=lr\n            num_hidden_nodes=num_hidden_nodes\n            DLGN_init= DLGN_FC(input_dim=input_dim, output_dim=1, num_hidden_nodes=num_hidden_nodes,beta=beta)\n            DLGN_obj_return,DLGN_obj_store=train_model_func(DLGN_init,trainloader,valloader,num_epoch,NPF_freeze,NPV_freeze)\n            del DLGN_init\n            del DLGN_obj_store\n\n    '''","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:47:43.046696Z","iopub.execute_input":"2023-09-28T20:47:43.047433Z","iopub.status.idle":"2023-09-28T20:48:13.012065Z","shell.execute_reply.started":"2023-09-28T20:47:43.047402Z","shell.execute_reply":"2023-09-28T20:48:13.010960Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"================= [125, 125, 125] =========================\n---------------------ADULT--------------------------------------\nNormalize dataset\nclasses [0 1]\nbest validation error rate (epoch 24): 0.15365489806066635\n\ntest error rate (model of epoch 24): 0.15301724137931033\n\ntest accuracy (model of epoch 24): 0.8469827586206897\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"markdown","source":"**DLGN Decision Tree Classifier**","metadata":{}},{"cell_type":"markdown","source":"**Node Class**","metadata":{}},{"cell_type":"code","source":"class Node():\n    def __init__(self, feature_vals=None,feature_bias=None, left=None, right=None, info_gain=None, value=None):\n        ''' constructor ''' \n        \n        # for decision node\n        self.feature_vals = feature_vals\n        self.feature_bias = feature_bias\n        self.left = left\n        self.right = right\n        self.info_gain = info_gain\n        \n        # for leaf node\n        self.value = value","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:48:13.013570Z","iopub.execute_input":"2023-09-28T20:48:13.014126Z","iopub.status.idle":"2023-09-28T20:48:13.020409Z","shell.execute_reply.started":"2023-09-28T20:48:13.014091Z","shell.execute_reply":"2023-09-28T20:48:13.019382Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Tree Class**","metadata":{}},{"cell_type":"code","source":"#2. Check the weights' (1500x500) 1st index (0th) if it is -ve make the entire row change sign (multiply by -1) and do experiments. So the 4th coordinate datas will now become 1st coordinate,\n#so will get everything in +ve w not -ve w. clustering of centres will happen for only +ve.\ndef process_array(arr):\n    mask = arr[:, 0] < 0\n    arr[mask] *= -1\n    return arr","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:48:13.021805Z","iopub.execute_input":"2023-09-28T20:48:13.022407Z","iopub.status.idle":"2023-09-28T20:48:13.037352Z","shell.execute_reply.started":"2023-09-28T20:48:13.022375Z","shell.execute_reply":"2023-09-28T20:48:13.036416Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def pick_bias(curr_data, W):\n    X, Y = curr_data[:, :-1], curr_data[:, -1]\n    scores = []\n    WX = X @ W.T\n    min_wx, max_wx = np.min(WX), np.max(WX)\n\n    print(\"min_wx:\",min_wx)\n    print(\"max_wx:\",max_wx)\n    b_store = np.arange(min_wx, max_wx, .1)\n    for b in b_store:\n        score = 0\n        condition = np.logical_and(WX + b >= -0.1, WX + b <= 0.1)\n        relevant_X = X[condition[:, 0]]\n\n        relevant_Y = Y[condition[:, 0]]\n        if len(relevant_X) > 0:  # Check if there are relevant data points\n            xi = relevant_X#.reshape(relevant_X.shape[0], 1, -1)\n            x_dash = xi - (xi @ W.T + b) / (W @ W.T) * W\n            distances = np.linalg.norm(x_dash - relevant_X, axis=1)\n            nearest_indices = np.argmin(distances)\n            xj = relevant_X[nearest_indices]\n            yj = relevant_Y[nearest_indices]\n\n            mismatched = relevant_Y != yj\n            score = np.sum(mismatched)\n\n        scores.append(score)\n    return b_store[np.argmax(scores)]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:48:13.038757Z","iopub.execute_input":"2023-09-28T20:48:13.039104Z","iopub.status.idle":"2023-09-28T20:48:13.054240Z","shell.execute_reply.started":"2023-09-28T20:48:13.039074Z","shell.execute_reply":"2023-09-28T20:48:13.053433Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class DLGNDecisionTreeClassifier():\n    def __init__(self, min_samples_split=2, max_depth=2):\n        ''' constructor '''\n        \n        # initialize the root of the tree \n        self.root = None\n        \n        # stopping conditions\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        \n    def build_tree(self, dataset,dataset_val,eps,min_samples,num_hidden_nodes, curr_depth=0,curr_node=0):\n        ''' recursive function to build the tree ''' \n        \n        X, Y = dataset[:,:-1], dataset[:,-1]\n        num_samples, num_features = np.shape(X)\n        print(\"curr_depth:\",curr_depth)\n        print(\"curr_node:\",curr_node)\n        print(\"num_samples:\",num_samples)\n        print(\"num_features:\",num_features)\n\n        # split until stopping conditions are met\n        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n            # find the best split\n            best_split = self.get_best_split(dataset,dataset_val,eps,min_samples,curr_depth,curr_node,num_hidden_nodes)\n            # check if information gain is positive\n            print(\"best_info_gain:\",best_split[\"info_gain\"])\n            if best_split[\"info_gain\"]>0:\n                # recur left\n                left_subtree = self.build_tree(best_split[\"dataset_left\"],dataset_val,eps,min_samples,num_hidden_nodes, curr_depth+1,2*curr_node+1)\n                # recur right\n                right_subtree = self.build_tree(best_split[\"dataset_right\"],dataset_val,eps,min_samples,num_hidden_nodes, curr_depth+1,2*curr_node+2)\n                # return decision node\n                return Node(best_split[\"feature_vals\"], best_split[\"feature_bias\"],\n                            left_subtree, right_subtree, best_split[\"info_gain\"])\n        \n        # compute leaf node\n        leaf_value = self.calculate_leaf_value(Y)\n        print(\"leaf_value:\",leaf_value)\n        # return leaf node\n        return Node(value=leaf_value)\n    \n    def get_best_split(self, dataset,dataset_val,eps,min_samples,curr_depth,curr_node,num_hidden_nodes):\n        ''' function to find the best split '''\n        # dictionary to store the best split\n        best_split = {}\n        max_info_gain = -float(\"inf\")\n        \n        \n        \n        \n        #USE DLGN FOR HYPERPLANE CLUSTERING\n#         num_hidden_nodes=[500,500,500]\n        X_train, Y_train = dataset[:,:-1], dataset[:,-1]\n        X_valid,Y_valid = dataset_val[:,:-1],dataset_val[:,-1]\n#         X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=.1, random_state=41)\n        print(\"X_train:\",X_train.shape)\n        set_npseed(seed)\n        set_torchseed(seed)\n\n        input_dim=X_train.shape[1]\n        trainloader = torch.utils.data.DataLoader(TorchDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=True)\n        valloader = torch.utils.data.DataLoader(TorchDataset(X_valid, Y_valid), batch_size=BATCH_SIZE, shuffle=False)\n\n        NPF_freeze = False #Default False, set True - when freeze the whole NPF\n        NPV_freeze = False #Default False, set True - when freeze the whole NPV\n\n        ## For single run\n        ##DLGN clustering of wts\n        DLGN_init= DLGN_FC(input_dim=input_dim, output_dim=1, num_hidden_nodes=num_hidden_nodes,beta=beta)\n        DLGN_obj_return,DLGN_obj_store=train_model_func(DLGN_init,trainloader,valloader,num_epoch,NPF_freeze,NPV_freeze)\n        \n        effective_weights, effective_biases = DLGN_obj_return.return_gating_functions()\n        wts_list=[] #DLGN model effective wts\n        for layer in range(len(effective_weights)):\n            if layer!=0:\n                wts =  np.array(effective_weights[layer].cpu().data.detach().numpy())\n                wts /= np.linalg.norm(wts, axis=1)[:,None]\n                wts_list.append(wts)\n        wts_list = np.concatenate(wts_list)\n        wts_list = process_array(wts_list)\n        print(\"wts_list shape:\",wts_list.shape)\n        \n        min_dis = np.inf\n        eps_best = 0\n        min_samples_best = 0\n        label_max_clus_best=0\n\n        core_samples, dbscan_labels = cluster.dbscan(wts_list, eps=eps, metric='euclidean', min_samples=min_samples)\n        num_clusters = max(dbscan_labels)+1\n        if num_clusters>0:\n            print(\"eps:\",eps,\"min_samples:\",min_samples)\n            cluster_centres = []\n            cluster_no = []\n            for i in range(num_clusters):\n                cluster_centres.append(wts_list[dbscan_labels==i].mean(axis=0))\n                cluster_no.append(len(wts_list[dbscan_labels==i]))\n            cluster_centres = np.array(cluster_centres)\n            label_max_clus = np.argmax(cluster_no)\n            feature_vals = cluster_centres[label_max_clus].reshape(1,-1)\n\n            feature_bias = pick_bias(dataset,feature_vals)\n            print(\"feature_bias:\",feature_bias)\n\n            print(\"cluster_centres_size:\",len(cluster_centres))\n            print(\"No of wts in each cluster:\",cluster_no)\n            print(\"cluster_no[label_max_clus]:\",cluster_no[label_max_clus])\n            print(\"label_max_clus:\",label_max_clus)\n            # get current split\n            dataset_left, dataset_right = self.split(dataset, feature_vals,feature_bias)\n            print(\"dataset_left:\",len(dataset_left))\n            print(\"dataset_right:\",len(dataset_right))\n        # check if childs are not null\n            if len(dataset_left)>0 and len(dataset_right)>0:\n                y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n                print(\"no of 0s in left:\",np.count_nonzero(left_y == 0))\n                print(\"no of 1s in left:\",np.count_nonzero(left_y == 1))\n                print(\"no of 0s in right:\",np.count_nonzero(right_y == 0))\n                print(\"no of 1s in right:\",np.count_nonzero(right_y == 1))\n                # compute information gain\n                curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n                print(\"curr_info_gain:\",curr_info_gain)\n                # update the best split if needed\n#                         if curr_info_gain>max_info_gain:\n                best_split[\"feature_vals\"] = feature_vals\n                best_split[\"feature_bias\"] = feature_bias\n                best_split[\"dataset_left\"] = dataset_left\n                best_split[\"dataset_right\"] = dataset_right\n                best_split[\"info_gain\"] = curr_info_gain\n\n                max_info_gain = curr_info_gain\n                eps_best = eps\n                min_samples_best = min_samples\n            else:\n                best_split[\"feature_vals\"] = 0\n                best_split[\"feature_bias\"] = 0\n                best_split[\"dataset_left\"] = 0\n                best_split[\"dataset_right\"] = 0\n                best_split[\"info_gain\"] = 0\n                    \n        else:\n            # dictionary to store the best split\n            print(\"============LAST LEVEL===========================\")\n           \n           \n#             # Create a logistic regression model\n            model = LogisticRegression()\n            # Check the number of unique classes\n            num_classes_y = len(np.unique(Y_train))\n            if num_classes_y<2:\n                if Y_train[-1]==0:\n                    Y_train[-1]=1\n                else:\n                    Y_train[-1]=0\n            # Fit the model to the data\n            model.fit(X_train, Y_train)\n\n            # Get the model coefficients (weights)\n            weights = model.coef_\n            bias = model.intercept_\n            print(\"Model Weights:\")\n            print(weights)\n            \n            feature_vals=weights[0]\n            feature_bias=bias[0]\n            print(\"feature_bias:\",feature_bias)\n            dataset_left, dataset_right = self.split(dataset, feature_vals,feature_bias)\n            print(\"dataset_left:\",len(dataset_left))\n            print(\"dataset_right:\",len(dataset_right))\n            # check if childs are not null\n            if len(dataset_left)>0 and len(dataset_right)>0:\n                y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n                print(\"no of 0s in left:\",np.count_nonzero(left_y == 0))\n                print(\"no of 1s in left:\",np.count_nonzero(left_y == 1))\n                print(\"no of 0s in right:\",np.count_nonzero(right_y == 0))\n                print(\"no of 1s in right:\",np.count_nonzero(right_y == 1))\n                # compute information gain\n                curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n                print(\"curr_info_gain:\",curr_info_gain)\n                # update the best split if needed\n                \n                best_split[\"feature_vals\"] = feature_vals\n                best_split[\"feature_bias\"] = feature_bias\n                best_split[\"dataset_left\"] = dataset_left\n                best_split[\"dataset_right\"] = dataset_right\n                best_split[\"info_gain\"] = curr_info_gain\n                max_info_gain = curr_info_gain\n            else:\n                best_split[\"feature_vals\"] = 0\n                best_split[\"feature_bias\"] = 0\n                best_split[\"dataset_left\"] = 0\n                best_split[\"dataset_right\"] = 0\n                best_split[\"info_gain\"] = 0\n        print(\"best_split_info_gain:\",best_split[\"info_gain\"])\n        return best_split\n    \n    def split(self, dataset, feature_vals,feature_bias):\n        ''' function to split the data '''\n        \n        dataset_left = np.array([row for row in dataset if ((row[:-1]@feature_vals.T)+feature_bias<0)])\n        dataset_right = np.array([row for row in dataset if ((row[:-1]@feature_vals.T)+feature_bias>=0)])\n        return dataset_left, dataset_right\n    \n    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n        ''' function to compute information gain '''\n        \n        weight_l = len(l_child) / len(parent)\n        weight_r = len(r_child) / len(parent)\n        if mode==\"gini\":\n            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n        else:\n            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n        return gain\n    \n    def entropy(self, y):\n        ''' function to compute entropy '''\n        \n        class_labels = np.unique(y)\n        entropy = 0\n        for cls in class_labels:\n            p_cls = len(y[y == cls]) / len(y)\n            entropy += -p_cls * np.log2(p_cls)\n        return entropy\n    \n    def gini_index(self, y):\n        ''' function to compute gini index '''\n        \n        class_labels = np.unique(y)\n        gini = 0\n        for cls in class_labels:\n            p_cls = len(y[y == cls]) / len(y)\n            gini += p_cls**2\n        return 1 - gini\n        \n    def calculate_leaf_value(self, Y):\n        ''' function to compute leaf node '''\n        \n        Y = list(Y)\n        return max(Y, key=Y.count)\n    \n    def print_tree(self, tree=None, indent=\" \"):\n        ''' function to print the tree '''\n        \n        if not tree:\n            tree = self.root\n\n        if tree.value is not None:\n            print(tree.value)\n\n        else:\n            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n            print(\"%sleft:\" % (indent), end=\"\")\n            self.print_tree(tree.left, indent + indent)\n            print(\"%sright:\" % (indent), end=\"\")\n            self.print_tree(tree.right, indent + indent)\n    \n    def fit(self, X, Y,X_val,Y_val,eps,min_samples,num_hidden_nodes):\n        ''' function to train the tree '''\n        \n        dataset = np.concatenate((X, Y), axis=1,dtype=np.float32)\n        dataset_val = np.concatenate((X_val, Y_val), axis=1,dtype=np.float32)\n        self.root = self.build_tree(dataset,dataset_val,eps,min_samples,num_hidden_nodes)\n    \n    def predict(self, X):\n        ''' function to predict new dataset '''\n        \n        preditions = [self.make_prediction(x, self.root) for x in X]\n        return preditions\n    \n    def make_prediction(self, x, tree):\n        ''' function to predict a single data point '''\n        \n        if tree.value!=None: return tree.value\n        feature_vals = tree.feature_vals\n        feature_bias = tree.feature_bias\n        if x@feature_vals.T+feature_bias<0:\n            return self.make_prediction(x, tree.left)\n        else:\n            return self.make_prediction(x, tree.right)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:48:13.055631Z","iopub.execute_input":"2023-09-28T20:48:13.056030Z","iopub.status.idle":"2023-09-28T20:48:13.106960Z","shell.execute_reply.started":"2023-09-28T20:48:13.055995Z","shell.execute_reply":"2023-09-28T20:48:13.106153Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"DATA_NAME=[\"ADULT\"]\n# DATA_NAME=[\"ADULT\",\"bank_marketing\",\"gamma_telescope\"]#,\"rice_dataset\",\"german_credit_data\",\"spambase_dataset\",\"accelerometer_gyro_dataset\",\"swarm_behaviour\"]#,\"german_credit_data\"]#,\"rice_dataset\"]\n# DATA_NAME=[\"gamma_telescope\"]#,\"bank_marketing\",\"credit_card_defaults\",\"gamma_telescope\",\"rice_dataset\",\"german_credit_data\",\"spambase_dataset\",\"accelerometer_gyro_dataset\",\"swarm_behaviour\"]#,\"HIGGS\"]\nnum_epoch_list=[256]\nnum_hidden_nodes_list=[[750,750,750]]#,[750,750,750],[500,500,500,500],[750,750,750,750]]\neps_list=.3\nmin_samples_list=5\n# num_hidden_nodes_list=[[115,115,115],[117,117,117],[119,119,119],[121,121,121],[123,123,123],[127,127,127],[129,129,129]]\nmax_depth_list = [6]\nfor data_name in DATA_NAME:\n    for num_epoch in num_epoch_list:\n        for num_hidden_nodes in num_hidden_nodes_list:\n            for max_depth in max_depth_list:\n                data = Dataset(data_name)\n                print('classes', np.unique(data.y_test))\n                print(num_epoch)\n                print(num_hidden_nodes)\n                print(max_depth)\n                clf = DLGNDecisionTreeClassifier(min_samples_split=int(0.02*data.X_train.shape[0]), max_depth=max_depth)\n                clf.fit(data.X_train, data.y_train.reshape(-1,1),data.X_valid,data.y_valid.reshape(-1,1),eps_list, min_samples_list,num_hidden_nodes)\n                y_pred_test = clf.predict(data.X_test)\n                test_accuracy = accuracy_score(data.y_test.reshape(-1,1), y_pred_test)\n                print(\"Test Accuracy:\", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:48:13.108267Z","iopub.execute_input":"2023-09-28T20:48:13.108734Z","iopub.status.idle":"2023-09-28T20:54:23.222318Z","shell.execute_reply.started":"2023-09-28T20:48:13.108702Z","shell.execute_reply":"2023-09-28T20:54:23.221209Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"---------------------ADULT--------------------------------------\nclasses [0 1]\n256\n[750, 750, 750]\n6\ncurr_depth: 0\ncurr_node: 0\nnum_samples: 18097\nnum_features: 14\nX_train: (18097, 14)\nbest validation error rate (epoch 10): 0.15150008287750705\n\ntest error rate (model of epoch 10): 0.15053050397877984\n\ntest accuracy (model of epoch 10): 0.8494694960212201\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -10.413251\nmax_wx: 1.1404443\nfeature_bias: -0.21325092315677452\ncluster_centres_size: 8\nNo of wts in each cluster: [214, 139, 13, 16, 7, 8, 5, 6]\ncluster_no[label_max_clus]: 214\nlabel_max_clus: 0\ndataset_left: 10882\ndataset_right: 7215\nno of 0s in left: 8871\nno of 1s in left: 2011\nno of 0s in right: 4755\nno of 1s in right: 2460\ncurr_info_gain: 0.01169170521020707\nbest_split_info_gain: 0.01169170521020707\nbest_info_gain: 0.01169170521020707\ncurr_depth: 1\ncurr_node: 1\nnum_samples: 10882\nnum_features: 14\nX_train: (10882, 14)\nbest validation error rate (epoch 16): 0.16127962870876844\n\ntest error rate (model of epoch 16): 0.16147214854111405\n\ntest accuracy (model of epoch 16): 0.838527851458886\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.9511303\nmax_wx: 9.907302\nfeature_bias: -0.35113027095794536\ncluster_centres_size: 5\nNo of wts in each cluster: [273, 142, 5, 5, 5]\ncluster_no[label_max_clus]: 273\nlabel_max_clus: 0\ndataset_left: 9488\ndataset_right: 1394\nno of 0s in left: 8597\nno of 1s in left: 891\nno of 0s in right: 274\nno of 1s in right: 1120\ncurr_info_gain: 0.11245996276326653\nbest_split_info_gain: 0.11245996276326653\nbest_info_gain: 0.11245996276326653\ncurr_depth: 2\ncurr_node: 3\nnum_samples: 9488\nnum_features: 14\nX_train: (9488, 14)\nbest validation error rate (epoch 7): 0.20520470744240013\n\ntest error rate (model of epoch 7): 0.200762599469496\n\ntest accuracy (model of epoch 7): 0.799237400530504\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.7532055\nmax_wx: 1.8347895\nfeature_bias: -0.1532055377960182\ncluster_centres_size: 5\nNo of wts in each cluster: [167, 9, 10, 9, 8]\ncluster_no[label_max_clus]: 167\nlabel_max_clus: 0\ndataset_left: 8526\ndataset_right: 962\nno of 0s in left: 8000\nno of 1s in left: 526\nno of 0s in right: 597\nno of 1s in right: 365\ncurr_info_gain: 0.018395082219735348\nbest_split_info_gain: 0.018395082219735348\nbest_info_gain: 0.018395082219735348\ncurr_depth: 3\ncurr_node: 7\nnum_samples: 8526\nnum_features: 14\nX_train: (8526, 14)\nbest validation error rate (epoch 25): 0.21117188794961048\n\ntest error rate (model of epoch 25): 0.21104111405835543\n\ntest accuracy (model of epoch 25): 0.7889588859416445\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.1547127\nmax_wx: 2.4475522\nfeature_bias: 0.3452873229980482\ncluster_centres_size: 1\nNo of wts in each cluster: [5]\ncluster_no[label_max_clus]: 5\nlabel_max_clus: 0\ndataset_left: 985\ndataset_right: 7541\nno of 0s in left: 857\nno of 1s in left: 128\nno of 0s in right: 7143\nno of 1s in right: 398\ncurr_info_gain: 0.001217065093839259\nbest_split_info_gain: 0.001217065093839259\nbest_info_gain: 0.001217065093839259\ncurr_depth: 4\ncurr_node: 15\nnum_samples: 985\nnum_features: 14\nX_train: (985, 14)\nbest validation error rate (epoch 5): 0.23388032487982763\n\ntest error rate (model of epoch 5): 0.23143236074270557\n\ntest accuracy (model of epoch 5): 0.7685676392572944\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -3.5494416\nmax_wx: 1.2707813\nfeature_bias: 0.6505584239959754\ncluster_centres_size: 1\nNo of wts in each cluster: [128]\ncluster_no[label_max_clus]: 128\nlabel_max_clus: 0\ndataset_left: 535\ndataset_right: 450\nno of 0s in left: 497\nno of 1s in left: 38\nno of 0s in right: 360\nno of 1s in right: 90\ncurr_info_gain: 0.008254950015207735\nbest_split_info_gain: 0.008254950015207735\nbest_info_gain: 0.008254950015207735\ncurr_depth: 5\ncurr_node: 31\nnum_samples: 535\nnum_features: 14\nX_train: (535, 14)\nbest validation error rate (epoch 4): 0.23653240510525445\n\ntest error rate (model of epoch 4): 0.22728779840848806\n\ntest accuracy (model of epoch 4): 0.772712201591512\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -5.285922\nmax_wx: 0.16688922\nfeature_bias: 0.1140779495239066\ncluster_centres_size: 1\nNo of wts in each cluster: [135]\ncluster_no[label_max_clus]: 135\nlabel_max_clus: 0\ndataset_left: 532\ndataset_right: 3\nno of 0s in left: 494\nno of 1s in left: 38\nno of 0s in right: 3\nno of 1s in right: 0\ncurr_info_gain: 5.6898294298751706e-05\nbest_split_info_gain: 5.6898294298751706e-05\nbest_info_gain: 5.6898294298751706e-05\ncurr_depth: 6\ncurr_node: 63\nnum_samples: 532\nnum_features: 14\nX_train: (532, 14)\nbest validation error rate (epoch 11): 0.23736118017570032\n\ntest error rate (model of epoch 11): 0.22645888594164457\n\ntest accuracy (model of epoch 11): 0.7735411140583555\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -4.98111\nmax_wx: 0.014258422\nfeature_bias: -4.981110095977783\ncluster_centres_size: 1\nNo of wts in each cluster: [479]\ncluster_no[label_max_clus]: 479\nlabel_max_clus: 0\ndataset_left: 532\ndataset_right: 0\nbest_split_info_gain: 0\nbest_info_gain: 0\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 64\nnum_samples: 3\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 5\ncurr_node: 32\nnum_samples: 450\nnum_features: 14\nX_train: (450, 14)\nbest validation error rate (epoch 10): 0.25012431626056686\n\ntest error rate (model of epoch 10): 0.24204244031830238\n\ntest accuracy (model of epoch 10): 0.7579575596816976\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -3.0135016\nmax_wx: 3.53514\nfeature_bias: 1.1864983558654822\ncluster_centres_size: 1\nNo of wts in each cluster: [5]\ncluster_no[label_max_clus]: 5\nlabel_max_clus: 0\ndataset_left: 163\ndataset_right: 287\nno of 0s in left: 143\nno of 1s in left: 20\nno of 0s in right: 217\nno of 1s in right: 70\ncurr_info_gain: 0.006787370941193971\nbest_split_info_gain: 0.006787370941193971\nbest_info_gain: 0.006787370941193971\ncurr_depth: 6\ncurr_node: 65\nnum_samples: 163\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 66\nnum_samples: 287\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 4\ncurr_node: 16\nnum_samples: 7541\nnum_features: 14\nX_train: (7541, 14)\nbest validation error rate (epoch 14): 0.20304989225924083\n\ntest error rate (model of epoch 14): 0.1939655172413793\n\ntest accuracy (model of epoch 14): 0.8060344827586207\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -4.156888\nmax_wx: 2.0970395\nfeature_bias: 0.34311199188230823\ncluster_centres_size: 9\nNo of wts in each cluster: [7, 7, 12, 15, 39, 29, 29, 5, 5]\ncluster_no[label_max_clus]: 39\nlabel_max_clus: 4\ndataset_left: 5221\ndataset_right: 2320\nno of 0s in left: 5135\nno of 1s in left: 86\nno of 0s in right: 2008\nno of 1s in right: 312\ncurr_info_gain: 0.005932768966686847\nbest_split_info_gain: 0.005932768966686847\nbest_info_gain: 0.005932768966686847\ncurr_depth: 5\ncurr_node: 33\nnum_samples: 5221\nnum_features: 14\nX_train: (5221, 14)\nbest validation error rate (epoch 0): 0.2575832918945798\n\ntest error rate (model of epoch 0): 0.2480106100795756\n\ntest accuracy (model of epoch 0): 0.7519893899204244\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -3.5730822\nmax_wx: 0.89018387\nfeature_bias: 0.026917791366580346\ncluster_centres_size: 1\nNo of wts in each cluster: [245]\ncluster_no[label_max_clus]: 245\nlabel_max_clus: 0\ndataset_left: 5108\ndataset_right: 113\nno of 0s in left: 5047\nno of 1s in left: 61\nno of 0s in right: 88\nno of 1s in right: 25\ncurr_info_gain: 0.00185514336076395\nbest_split_info_gain: 0.00185514336076395\nbest_info_gain: 0.00185514336076395\ncurr_depth: 6\ncurr_node: 67\nnum_samples: 5108\nnum_features: 14\nX_train: (5108, 14)\nbest validation error rate (epoch 12): 0.23238852975302504\n\ntest error rate (model of epoch 12): 0.21816976127320956\n\ntest accuracy (model of epoch 12): 0.7818302387267905\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -3.702962\nmax_wx: 0.59977335\nfeature_bias: 0.39703807830810867\ncluster_centres_size: 8\nNo of wts in each cluster: [16, 5, 8, 563, 5, 7, 6, 7]\ncluster_no[label_max_clus]: 563\nlabel_max_clus: 3\ndataset_left: 4859\ndataset_right: 249\nno of 0s in left: 4817\nno of 1s in left: 42\nno of 0s in right: 230\nno of 1s in right: 19\ncurr_info_gain: 0.0004245778019405441\nbest_split_info_gain: 0.0004245778019405441\nbest_info_gain: 0.0004245778019405441\ncurr_depth: 7\ncurr_node: 135\nnum_samples: 4859\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 136\nnum_samples: 249\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 68\nnum_samples: 113\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 5\ncurr_node: 34\nnum_samples: 2320\nnum_features: 14\nX_train: (2320, 14)\nbest validation error rate (epoch 20): 0.22658710425990386\n\ntest error rate (model of epoch 20): 0.20921750663129973\n\ntest accuracy (model of epoch 20): 0.7907824933687002\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.7618761\nmax_wx: 1.3543139\nfeature_bias: 0.03812389373779457\ncluster_centres_size: 6\nNo of wts in each cluster: [6, 110, 7, 8, 4, 5]\ncluster_no[label_max_clus]: 110\nlabel_max_clus: 1\ndataset_left: 1644\ndataset_right: 676\nno of 0s in left: 1507\nno of 1s in left: 137\nno of 0s in right: 501\nno of 1s in right: 175\ncurr_info_gain: 0.012725258978345921\nbest_split_info_gain: 0.012725258978345921\nbest_info_gain: 0.012725258978345921\ncurr_depth: 6\ncurr_node: 69\nnum_samples: 1644\nnum_features: 14\nX_train: (1644, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 1.7784  0.023   0.066   0.0442  0.9902 -1.7414  0.0862 -0.2075  0.1833\n   0.0395  2.5904  0.2947  0.3902  0.0211]]\nfeature_bias: -0.5411027898922888\ndataset_left: 1640\ndataset_right: 4\nno of 0s in left: 1505\nno of 1s in left: 135\nno of 0s in right: 2\nno of 1s in right: 2\ncurr_info_gain: 0.0008468834688348248\nbest_split_info_gain: 0.0008468834688348248\nbest_info_gain: 0.0008468834688348248\ncurr_depth: 7\ncurr_node: 139\nnum_samples: 1640\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 140\nnum_samples: 4\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 6\ncurr_node: 70\nnum_samples: 676\nnum_features: 14\nX_train: (676, 14)\nbest validation error rate (epoch 37): 0.19907177192110062\n\ntest error rate (model of epoch 37): 0.1937997347480106\n\ntest accuracy (model of epoch 37): 0.8062002652519894\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.8333132\nmax_wx: 2.4330015\nfeature_bias: -0.23331322669982768\ncluster_centres_size: 11\nNo of wts in each cluster: [105, 51, 5, 20, 33, 7, 12, 16, 5, 10, 4]\ncluster_no[label_max_clus]: 105\nlabel_max_clus: 0\ndataset_left: 341\ndataset_right: 335\nno of 0s in left: 283\nno of 1s in left: 58\nno of 0s in right: 218\nno of 1s in right: 117\ncurr_info_gain: 0.01604891943369996\nbest_split_info_gain: 0.01604891943369996\nbest_info_gain: 0.01604891943369996\ncurr_depth: 7\ncurr_node: 141\nnum_samples: 341\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 142\nnum_samples: 335\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 3\ncurr_node: 8\nnum_samples: 962\nnum_features: 14\nX_train: (962, 14)\nbest validation error rate (epoch 0): 0.2570860268523123\n\ntest error rate (model of epoch 0): 0.2448607427055703\n\ntest accuracy (model of epoch 0): 0.7551392572944298\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[-0.1778 -0.113   0.1632 -0.2207  0.6232 -0.167  -0.1721 -0.1317 -0.1216\n   0.2619  1.1935  0.1194  0.0216  0.0689]]\nfeature_bias: -0.8647266994417075\ndataset_left: 726\ndataset_right: 236\nno of 0s in left: 506\nno of 1s in left: 220\nno of 0s in right: 91\nno of 1s in right: 145\ncurr_info_gain: 0.03590046513511047\nbest_split_info_gain: 0.03590046513511047\nbest_info_gain: 0.03590046513511047\ncurr_depth: 4\ncurr_node: 17\nnum_samples: 726\nnum_features: 14\nX_train: (726, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[-0.1109 -0.144   0.1728 -0.254   0.5925 -0.2275 -0.1077 -0.0832 -0.0287\n   0.3265 -0.5735  0.0877  0.0315  0.0702]]\nfeature_bias: -1.0671113930689007\ndataset_left: 708\ndataset_right: 18\nno of 0s in left: 496\nno of 1s in left: 212\nno of 0s in right: 10\nno of 1s in right: 8\ncurr_info_gain: 0.0010168453928083299\nbest_split_info_gain: 0.0010168453928083299\nbest_info_gain: 0.0010168453928083299\ncurr_depth: 5\ncurr_node: 35\nnum_samples: 708\nnum_features: 14\nX_train: (708, 14)\nbest validation error rate (epoch 1): 0.2564230067959556\n\ntest error rate (model of epoch 1): 0.2445291777188329\n\ntest accuracy (model of epoch 1): 0.7554708222811671\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[-0.1188 -0.1454  0.1749 -0.2863  0.6237 -0.2344 -0.102  -0.1124 -0.0294\n   0.3341 -0.5726  0.0889  0.0377  0.0831]]\nfeature_bias: -1.0732194770798325\ndataset_left: 690\ndataset_right: 18\nno of 0s in left: 489\nno of 1s in left: 201\nno of 0s in right: 7\nno of 1s in right: 11\ncurr_info_gain: 0.005068277250184161\nbest_split_info_gain: 0.005068277250184161\nbest_info_gain: 0.005068277250184161\ncurr_depth: 6\ncurr_node: 71\nnum_samples: 690\nnum_features: 14\nX_train: (690, 14)\nbest validation error rate (epoch 9): 0.2575832918945798\n\ntest error rate (model of epoch 9): 0.245026525198939\n\ntest accuracy (model of epoch 9): 0.754973474801061\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -3.4535568\nmax_wx: 2.3127594\nfeature_bias: 0.34644322395325045\ncluster_centres_size: 4\nNo of wts in each cluster: [108, 10, 7, 5]\ncluster_no[label_max_clus]: 108\nlabel_max_clus: 0\ndataset_left: 394\ndataset_right: 296\nno of 0s in left: 265\nno of 1s in left: 129\nno of 0s in right: 224\nno of 1s in right: 72\ncurr_info_gain: 0.0034706672742234046\nbest_split_info_gain: 0.0034706672742234046\nbest_info_gain: 0.0034706672742234046\ncurr_depth: 7\ncurr_node: 143\nnum_samples: 394\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 144\nnum_samples: 296\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 72\nnum_samples: 18\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 5\ncurr_node: 36\nnum_samples: 18\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 4\ncurr_node: 18\nnum_samples: 236\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 2\ncurr_node: 4\nnum_samples: 1394\nnum_features: 14\nX_train: (1394, 14)\nbest validation error rate (epoch 12): 0.21349245814685894\n\ntest error rate (model of epoch 12): 0.20855437665782495\n\ntest accuracy (model of epoch 12): 0.791445623342175\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -12.814235\nmax_wx: 0.60577697\nfeature_bias: 0.2857652664184105\ncluster_centres_size: 3\nNo of wts in each cluster: [473, 374, 5]\ncluster_no[label_max_clus]: 473\nlabel_max_clus: 0\ndataset_left: 943\ndataset_right: 451\nno of 0s in left: 77\nno of 1s in left: 866\nno of 0s in right: 197\nno of 1s in right: 254\ncurr_info_gain: 0.055210689391039514\nbest_split_info_gain: 0.055210689391039514\nbest_info_gain: 0.055210689391039514\ncurr_depth: 3\ncurr_node: 9\nnum_samples: 943\nnum_features: 14\nX_train: (943, 14)\nbest validation error rate (epoch 41): 0.25725178186640146\n\ntest error rate (model of epoch 41): 0.2461870026525199\n\ntest accuracy (model of epoch 41): 0.7538129973474801\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.3710959\nmax_wx: 10.920907\nfeature_bias: -0.4710958957672111\ncluster_centres_size: 12\nNo of wts in each cluster: [269, 8, 5, 4, 53, 50, 15, 29, 9, 12, 4, 6]\ncluster_no[label_max_clus]: 269\nlabel_max_clus: 0\ndataset_left: 191\ndataset_right: 752\nno of 0s in left: 62\nno of 1s in left: 129\nno of 0s in right: 15\nno of 1s in right: 737\ncurr_info_gain: 0.029984042159124924\nbest_split_info_gain: 0.029984042159124924\nbest_info_gain: 0.029984042159124924\ncurr_depth: 4\ncurr_node: 19\nnum_samples: 191\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 4\ncurr_node: 20\nnum_samples: 752\nnum_features: 14\nX_train: (752, 14)\nbest validation error rate (epoch 91): 0.26156141223272006\n\ntest error rate (model of epoch 91): 0.26657824933687\n\ntest accuracy (model of epoch 91): 0.73342175066313\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -0.43512675\nmax_wx: 11.216718\nfeature_bias: -0.3351267516613007\ncluster_centres_size: 7\nNo of wts in each cluster: [722, 10, 23, 27, 7, 100, 7]\ncluster_no[label_max_clus]: 722\nlabel_max_clus: 0\ndataset_left: 36\ndataset_right: 716\nno of 0s in left: 8\nno of 1s in left: 28\nno of 0s in right: 7\nno of 1s in right: 709\ncurr_info_gain: 0.004114392032734518\nbest_split_info_gain: 0.004114392032734518\nbest_info_gain: 0.004114392032734518\ncurr_depth: 5\ncurr_node: 41\nnum_samples: 36\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 5\ncurr_node: 42\nnum_samples: 716\nnum_features: 14\nX_train: (716, 14)\nbest validation error rate (epoch 251): 0.46974970992872533\n\ntest error rate (model of epoch 251): 0.4744694960212202\n\ntest accuracy (model of epoch 251): 0.5255305039787799\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -0.033571385\nmax_wx: 12.071029\nfeature_bias: -0.033571384847164154\ncluster_centres_size: 7\nNo of wts in each cluster: [673, 39, 141, 5, 10, 5, 6]\ncluster_no[label_max_clus]: 673\nlabel_max_clus: 0\ndataset_left: 2\ndataset_right: 714\nno of 0s in left: 2\nno of 1s in left: 0\nno of 0s in right: 5\nno of 1s in right: 709\ncurr_info_gain: 0.00549323533914043\nbest_split_info_gain: 0.00549323533914043\nbest_info_gain: 0.00549323533914043\ncurr_depth: 6\ncurr_node: 85\nnum_samples: 2\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 86\nnum_samples: 714\nnum_features: 14\nX_train: (714, 14)\nbest validation error rate (epoch 7): 0.7420851980772418\n\ntest error rate (model of epoch 7): 0.7534814323607427\n\ntest accuracy (model of epoch 7): 0.24651856763925728\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: 0.20822552\nmax_wx: 12.583973\nfeature_bias: 0.20822551846504211\ncluster_centres_size: 2\nNo of wts in each cluster: [448, 269]\ncluster_no[label_max_clus]: 448\nlabel_max_clus: 0\ndataset_left: 0\ndataset_right: 714\nbest_split_info_gain: 0\nbest_info_gain: 0\nleaf_value: 1.0\ncurr_depth: 3\ncurr_node: 10\nnum_samples: 451\nnum_features: 14\nX_train: (451, 14)\nbest validation error rate (epoch 11): 0.2242665340626554\n\ntest error rate (model of epoch 11): 0.22248010610079574\n\ntest accuracy (model of epoch 11): 0.7775198938992043\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.6083283\nmax_wx: 4.0668216\nfeature_bias: -0.5083283424377432\ncluster_centres_size: 2\nNo of wts in each cluster: [212, 6]\ncluster_no[label_max_clus]: 212\nlabel_max_clus: 0\ndataset_left: 229\ndataset_right: 222\nno of 0s in left: 140\nno of 1s in left: 89\nno of 0s in right: 57\nno of 1s in right: 165\ncurr_info_gain: 0.06285435478086493\nbest_split_info_gain: 0.06285435478086493\nbest_info_gain: 0.06285435478086493\ncurr_depth: 4\ncurr_node: 21\nnum_samples: 229\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 4\ncurr_node: 22\nnum_samples: 222\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 1\ncurr_node: 2\nnum_samples: 7215\nnum_features: 14\nX_train: (7215, 14)\nbest validation error rate (epoch 19): 0.24680921597878336\n\ntest error rate (model of epoch 19): 0.2460212201591512\n\ntest accuracy (model of epoch 19): 0.7539787798408488\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.5178022\nmax_wx: 2.845773\nfeature_bias: -0.7178022384643548\ncluster_centres_size: 12\nNo of wts in each cluster: [8, 29, 13, 18, 10, 6, 34, 8, 10, 5, 4, 5]\ncluster_no[label_max_clus]: 34\nlabel_max_clus: 6\ndataset_left: 3825\ndataset_right: 3390\nno of 0s in left: 2211\nno of 1s in left: 1614\nno of 0s in right: 2544\nno of 1s in right: 846\ncurr_info_gain: 0.014807420886089662\nbest_split_info_gain: 0.014807420886089662\nbest_info_gain: 0.014807420886089662\ncurr_depth: 2\ncurr_node: 5\nnum_samples: 3825\nnum_features: 14\nX_train: (3825, 14)\nbest validation error rate (epoch 18): 0.21829935355544505\n\ntest error rate (model of epoch 18): 0.21021220159151194\n\ntest accuracy (model of epoch 18): 0.789787798408488\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.5101194\nmax_wx: 2.6094868\nfeature_bias: 0.18988056182861568\ncluster_centres_size: 10\nNo of wts in each cluster: [199, 5, 14, 8, 9, 5, 5, 8, 7, 5]\ncluster_no[label_max_clus]: 199\nlabel_max_clus: 0\ndataset_left: 1700\ndataset_right: 2125\nno of 0s in left: 1322\nno of 1s in left: 378\nno of 0s in right: 889\nno of 1s in right: 1236\ncurr_info_gain: 0.06374926566705114\nbest_split_info_gain: 0.06374926566705114\nbest_info_gain: 0.06374926566705114\ncurr_depth: 3\ncurr_node: 11\nnum_samples: 1700\nnum_features: 14\nX_train: (1700, 14)\nbest validation error rate (epoch 1): 0.2831095640643129\n\ntest error rate (model of epoch 1): 0.2619363395225464\n\ntest accuracy (model of epoch 1): 0.7380636604774535\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.30553\nmax_wx: 3.965371\nfeature_bias: 0.4944700479507462\ncluster_centres_size: 2\nNo of wts in each cluster: [23, 8]\ncluster_no[label_max_clus]: 23\nlabel_max_clus: 0\ndataset_left: 301\ndataset_right: 1399\nno of 0s in left: 162\nno of 1s in left: 139\nno of 0s in right: 1160\nno of 1s in right: 239\ncurr_info_gain: 0.024670394694929654\nbest_split_info_gain: 0.024670394694929654\nbest_info_gain: 0.024670394694929654\ncurr_depth: 4\ncurr_node: 23\nnum_samples: 301\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 4\ncurr_node: 24\nnum_samples: 1399\nnum_features: 14\nX_train: (1399, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 0.3677 -0.0674  0.1268 -0.0472  0.9647  2.5861 -0.1134  0.6543  0.1175\n   0.5293  0.0212  0.1946  0.651   0.0463]]\nfeature_bias: 0.705207012543557\ndataset_left: 1360\ndataset_right: 39\nno of 0s in left: 1141\nno of 1s in left: 219\nno of 0s in right: 19\nno of 1s in right: 20\ncurr_info_gain: 0.006707609647246537\nbest_split_info_gain: 0.006707609647246537\nbest_info_gain: 0.006707609647246537\ncurr_depth: 5\ncurr_node: 49\nnum_samples: 1360\nnum_features: 14\nX_train: (1360, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 0.3393 -0.1214  0.1144 -0.0597  1.0434  2.7206 -0.1019  0.6706  0.0645\n   0.5148  0.0265 -0.555   0.6645  0.0051]]\nfeature_bias: 0.6826441538228007\ndataset_left: 1343\ndataset_right: 17\nno of 0s in left: 1131\nno of 1s in left: 212\nno of 0s in right: 10\nno of 1s in right: 7\ncurr_info_gain: 0.0015915996890194517\nbest_split_info_gain: 0.0015915996890194517\nbest_info_gain: 0.0015915996890194517\ncurr_depth: 6\ncurr_node: 99\nnum_samples: 1343\nnum_features: 14\nX_train: (1343, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 3.5128e-01 -1.0720e-01  1.2628e-01 -6.0203e-02  1.0651e+00  2.7941e+00\n  -9.3198e-02  6.8973e-01  2.4042e-02  5.1428e-01  3.9918e-02 -5.5673e-01\n   7.0973e-01 -1.4491e-03]]\nfeature_bias: 0.7633626435853553\ndataset_left: 1336\ndataset_right: 7\nno of 0s in left: 1128\nno of 1s in left: 208\nno of 0s in right: 3\nno of 1s in right: 4\ncurr_info_gain: 0.0017923630600463203\nbest_split_info_gain: 0.0017923630600463203\nbest_info_gain: 0.0017923630600463203\ncurr_depth: 7\ncurr_node: 199\nnum_samples: 1336\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 200\nnum_samples: 7\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 6\ncurr_node: 100\nnum_samples: 17\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 5\ncurr_node: 50\nnum_samples: 39\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 3\ncurr_node: 12\nnum_samples: 2125\nnum_features: 14\nX_train: (2125, 14)\nbest validation error rate (epoch 31): 0.2771423835571026\n\ntest error rate (model of epoch 31): 0.27619363395225466\n\ntest accuracy (model of epoch 31): 0.7238063660477454\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.817333\nmax_wx: 2.9642088\nfeature_bias: 0.0826670169830348\ncluster_centres_size: 9\nNo of wts in each cluster: [247, 81, 5, 30, 12, 10, 13, 5, 5]\ncluster_no[label_max_clus]: 247\nlabel_max_clus: 0\ndataset_left: 1379\ndataset_right: 746\nno of 0s in left: 449\nno of 1s in left: 930\nno of 0s in right: 440\nno of 1s in right: 306\ncurr_info_gain: 0.031807322476943956\nbest_split_info_gain: 0.031807322476943956\nbest_info_gain: 0.031807322476943956\ncurr_depth: 4\ncurr_node: 25\nnum_samples: 1379\nnum_features: 14\nX_train: (1379, 14)\nbest validation error rate (epoch 28): 0.2791314437261727\n\ntest error rate (model of epoch 28): 0.29144562334217505\n\ntest accuracy (model of epoch 28): 0.708554376657825\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.6991465\nmax_wx: 5.1207385\nfeature_bias: -0.5991465091705304\ncluster_centres_size: 18\nNo of wts in each cluster: [16, 8, 7, 43, 5, 12, 8, 22, 5, 8, 12, 6, 8, 4, 4, 5, 7, 5]\ncluster_no[label_max_clus]: 43\nlabel_max_clus: 3\ndataset_left: 868\ndataset_right: 511\nno of 0s in left: 292\nno of 1s in left: 576\nno of 0s in right: 157\nno of 1s in right: 354\ncurr_info_gain: 0.0003967901218571157\nbest_split_info_gain: 0.0003967901218571157\nbest_info_gain: 0.0003967901218571157\ncurr_depth: 5\ncurr_node: 51\nnum_samples: 868\nnum_features: 14\nX_train: (868, 14)\nbest validation error rate (epoch 4): 0.3502403447704293\n\ntest error rate (model of epoch 4): 0.3647214854111406\n\ntest accuracy (model of epoch 4): 0.6352785145888594\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 0.0768  0.1357  0.1108  0.0118  0.5451 -1.518   0.1009 -0.0634 -0.0293\n  -0.0747 -0.2386  0.0746  0.465  -0.0062]]\nfeature_bias: -0.2333720960131789\ndataset_left: 86\ndataset_right: 782\nno of 0s in left: 43\nno of 1s in left: 43\nno of 0s in right: 249\nno of 1s in right: 533\ncurr_info_gain: 0.005886524179413433\nbest_split_info_gain: 0.005886524179413433\nbest_info_gain: 0.005886524179413433\ncurr_depth: 6\ncurr_node: 103\nnum_samples: 86\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 6\ncurr_node: 104\nnum_samples: 782\nnum_features: 14\nX_train: (782, 14)\nbest validation error rate (epoch 42): 0.38554616277142384\n\ntest error rate (model of epoch 42): 0.39605437665782495\n\ntest accuracy (model of epoch 42): 0.603945623342175\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.1695551\nmax_wx: 2.5114172\nfeature_bias: -0.36955506801605154\ncluster_centres_size: 18\nNo of wts in each cluster: [116, 12, 60, 36, 5, 28, 27, 20, 29, 5, 12, 8, 13, 8, 8, 8, 3, 5]\ncluster_no[label_max_clus]: 116\nlabel_max_clus: 0\ndataset_left: 432\ndataset_right: 350\nno of 0s in left: 150\nno of 1s in left: 282\nno of 0s in right: 99\nno of 1s in right: 251\ncurr_info_gain: 0.0020486553344790104\nbest_split_info_gain: 0.0020486553344790104\nbest_info_gain: 0.0020486553344790104\ncurr_depth: 7\ncurr_node: 209\nnum_samples: 432\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 7\ncurr_node: 210\nnum_samples: 350\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 5\ncurr_node: 52\nnum_samples: 511\nnum_features: 14\nX_train: (511, 14)\nbest validation error rate (epoch 49): 0.4206862257583292\n\ntest error rate (model of epoch 49): 0.40484084880636606\n\ntest accuracy (model of epoch 49): 0.5951591511936339\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.735305\nmax_wx: 2.3453565\nfeature_bias: -0.9353050708770736\ncluster_centres_size: 13\nNo of wts in each cluster: [66, 32, 19, 128, 86, 5, 6, 15, 51, 22, 11, 7, 7]\ncluster_no[label_max_clus]: 128\nlabel_max_clus: 3\ndataset_left: 345\ndataset_right: 166\nno of 0s in left: 98\nno of 1s in left: 247\nno of 0s in right: 59\nno of 1s in right: 107\ncurr_info_gain: 0.002233933741052685\nbest_split_info_gain: 0.002233933741052685\nbest_info_gain: 0.002233933741052685\ncurr_depth: 6\ncurr_node: 105\nnum_samples: 345\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 6\ncurr_node: 106\nnum_samples: 166\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 4\ncurr_node: 26\nnum_samples: 746\nnum_features: 14\nX_train: (746, 14)\nbest validation error rate (epoch 10): 0.24747223603514007\n\ntest error rate (model of epoch 10): 0.23640583554376657\n\ntest accuracy (model of epoch 10): 0.7635941644562334\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.1320207\nmax_wx: 3.8850086\nfeature_bias: 0.06797928810119735\ncluster_centres_size: 2\nNo of wts in each cluster: [471, 142]\ncluster_no[label_max_clus]: 471\nlabel_max_clus: 0\ndataset_left: 467\ndataset_right: 279\nno of 0s in left: 243\nno of 1s in left: 224\nno of 0s in right: 197\nno of 1s in right: 82\ncurr_info_gain: 0.01615599723181721\nbest_split_info_gain: 0.01615599723181721\nbest_info_gain: 0.01615599723181721\ncurr_depth: 5\ncurr_node: 53\nnum_samples: 467\nnum_features: 14\nX_train: (467, 14)\nbest validation error rate (epoch 23): 0.22526106414719047\n\ntest error rate (model of epoch 23): 0.20921750663129973\n\ntest accuracy (model of epoch 23): 0.7907824933687002\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -0.5695999\nmax_wx: 0.7614099\nfeature_bias: -0.1695999264717103\ncluster_centres_size: 1\nNo of wts in each cluster: [709]\ncluster_no[label_max_clus]: 709\nlabel_max_clus: 0\ndataset_left: 233\ndataset_right: 234\nno of 0s in left: 134\nno of 1s in left: 99\nno of 0s in right: 109\nno of 1s in right: 125\ncurr_info_gain: 0.005972707228781682\nbest_split_info_gain: 0.005972707228781682\nbest_info_gain: 0.005972707228781682\ncurr_depth: 6\ncurr_node: 107\nnum_samples: 233\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 108\nnum_samples: 234\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 5\ncurr_node: 54\nnum_samples: 279\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 2\ncurr_node: 6\nnum_samples: 3390\nnum_features: 14\nX_train: (3390, 14)\nbest validation error rate (epoch 23): 0.23785844521796784\n\ntest error rate (model of epoch 23): 0.2349137931034483\n\ntest accuracy (model of epoch 23): 0.7650862068965517\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.0290673\nmax_wx: 4.2870026\nfeature_bias: -0.629067277908324\ncluster_centres_size: 10\nNo of wts in each cluster: [231, 5, 54, 33, 5, 5, 34, 6, 7, 6]\ncluster_no[label_max_clus]: 231\nlabel_max_clus: 0\ndataset_left: 1628\ndataset_right: 1762\nno of 0s in left: 942\nno of 1s in left: 686\nno of 0s in right: 1602\nno of 1s in right: 160\ncurr_info_gain: 0.05455289836486882\nbest_split_info_gain: 0.05455289836486882\nbest_info_gain: 0.05455289836486882\ncurr_depth: 3\ncurr_node: 13\nnum_samples: 1628\nnum_features: 14\nX_train: (1628, 14)\nbest validation error rate (epoch 0): 0.26669981766948453\n\ntest error rate (model of epoch 0): 0.2675729442970822\n\ntest accuracy (model of epoch 0): 0.7324270557029178\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.1609762\nmax_wx: 5.2937994\nfeature_bias: -0.46097617149352876\ncluster_centres_size: 2\nNo of wts in each cluster: [67, 8]\ncluster_no[label_max_clus]: 67\nlabel_max_clus: 0\ndataset_left: 676\ndataset_right: 952\nno of 0s in left: 513\nno of 1s in left: 163\nno of 0s in right: 429\nno of 1s in right: 523\ncurr_info_gain: 0.04614219966246724\nbest_split_info_gain: 0.04614219966246724\nbest_info_gain: 0.04614219966246724\ncurr_depth: 4\ncurr_node: 27\nnum_samples: 676\nnum_features: 14\nX_train: (676, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 3.9979e-01 -5.9257e-02  2.3879e-01 -3.5263e-04  1.4367e+00  1.7678e+00\n   4.1014e-02  6.2922e-02 -3.8709e-02  4.2320e-01 -4.7440e-01 -2.8806e-01\n   4.5082e-01  6.3608e-02]]\nfeature_bias: -0.08958861641388914\ndataset_left: 618\ndataset_right: 58\nno of 0s in left: 487\nno of 1s in left: 131\nno of 0s in right: 26\nno of 1s in right: 32\ncurr_info_gain: 0.01810806624308503\nbest_split_info_gain: 0.01810806624308503\nbest_info_gain: 0.01810806624308503\ncurr_depth: 5\ncurr_node: 55\nnum_samples: 618\nnum_features: 14\nX_train: (618, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 0.4663 -0.0577  0.1784 -0.0534  1.4045  1.7903  0.0371  0.1726 -0.0364\n   0.3869 -0.3212 -0.2938  0.4972  0.0573]]\nfeature_bias: -0.014559718150755354\ndataset_left: 612\ndataset_right: 6\nno of 0s in left: 485\nno of 1s in left: 127\nno of 0s in right: 2\nno of 1s in right: 4\ncurr_info_gain: 0.004053829824679223\nbest_split_info_gain: 0.004053829824679223\nbest_info_gain: 0.004053829824679223\ncurr_depth: 6\ncurr_node: 111\nnum_samples: 612\nnum_features: 14\nX_train: (612, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 0.4594 -0.0443  0.1855 -0.0853  1.3661  1.8112  0.035   0.2564 -0.0376\n   0.3678 -0.3023 -0.3035  0.5037  0.0528]]\nfeature_bias: 0.06294863376099194\ndataset_left: 611\ndataset_right: 1\nno of 0s in left: 484\nno of 1s in left: 127\nno of 0s in right: 1\nno of 1s in right: 0\ncurr_info_gain: 0.00014095918596657153\nbest_split_info_gain: 0.00014095918596657153\nbest_info_gain: 0.00014095918596657153\ncurr_depth: 7\ncurr_node: 223\nnum_samples: 611\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 224\nnum_samples: 1\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 112\nnum_samples: 6\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 5\ncurr_node: 56\nnum_samples: 58\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 4\ncurr_node: 28\nnum_samples: 952\nnum_features: 14\nX_train: (952, 14)\nbest validation error rate (epoch 4): 0.2650422675285927\n\ntest error rate (model of epoch 4): 0.2625994694960212\n\ntest accuracy (model of epoch 4): 0.7374005305039788\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.7265258\nmax_wx: 1.0917416\nfeature_bias: 0.2734742164611843\ncluster_centres_size: 2\nNo of wts in each cluster: [270, 41]\ncluster_no[label_max_clus]: 270\nlabel_max_clus: 0\ndataset_left: 478\ndataset_right: 474\nno of 0s in left: 146\nno of 1s in left: 332\nno of 0s in right: 283\nno of 1s in right: 191\ncurr_info_gain: 0.04251659480935027\nbest_split_info_gain: 0.04251659480935027\nbest_info_gain: 0.04251659480935027\ncurr_depth: 5\ncurr_node: 57\nnum_samples: 478\nnum_features: 14\nX_train: (478, 14)\nbest validation error rate (epoch 22): 0.2647107575004144\n\ntest error rate (model of epoch 22): 0.2524867374005305\n\ntest accuracy (model of epoch 22): 0.7475132625994695\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -2.8323433\nmax_wx: 0.72359407\nfeature_bias: 0.5676566600799591\ncluster_centres_size: 5\nNo of wts in each cluster: [166, 5, 14, 7, 10]\ncluster_no[label_max_clus]: 166\nlabel_max_clus: 0\ndataset_left: 358\ndataset_right: 120\nno of 0s in left: 96\nno of 1s in left: 262\nno of 0s in right: 50\nno of 1s in right: 70\ncurr_info_gain: 0.008293756626341575\nbest_split_info_gain: 0.008293756626341575\nbest_info_gain: 0.008293756626341575\ncurr_depth: 6\ncurr_node: 115\nnum_samples: 358\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 6\ncurr_node: 116\nnum_samples: 120\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 5\ncurr_node: 58\nnum_samples: 474\nnum_features: 14\nX_train: (474, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 0.0237 -0.2     0.0975  0.0446  0.991   1.3888  0.1696  0.0723 -0.1118\n   0.614  -0.1415  0.2933  0.2818 -0.0774]]\nfeature_bias: -0.32604757254301747\ndataset_left: 364\ndataset_right: 110\nno of 0s in left: 232\nno of 1s in left: 132\nno of 0s in right: 51\nno of 1s in right: 59\ncurr_info_gain: 0.010757178871263151\nbest_split_info_gain: 0.010757178871263151\nbest_info_gain: 0.010757178871263151\ncurr_depth: 6\ncurr_node: 117\nnum_samples: 364\nnum_features: 14\nX_train: (364, 14)\nbest validation error rate (epoch 0): 0.257749046908669\n\ntest error rate (model of epoch 0): 0.24568965517241378\n\ntest accuracy (model of epoch 0): 0.7543103448275862\n\nwts_list shape: (1500, 14)\n============LAST LEVEL===========================\nModel Weights:\n[[ 0.0419 -0.3131  0.0999  0.0667  1.3016  1.6301  0.3169  0.1035 -0.0964\n   0.6434 -0.1444  0.2609  0.3283 -0.0291]]\nfeature_bias: -0.1171656338918057\ndataset_left: 308\ndataset_right: 56\nno of 0s in left: 208\nno of 1s in left: 100\nno of 0s in right: 24\nno of 1s in right: 32\ncurr_info_gain: 0.015852279588543294\nbest_split_info_gain: 0.015852279588543294\nbest_info_gain: 0.015852279588543294\ncurr_depth: 7\ncurr_node: 235\nnum_samples: 308\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 236\nnum_samples: 56\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 6\ncurr_node: 118\nnum_samples: 110\nnum_features: 14\nleaf_value: 1.0\ncurr_depth: 3\ncurr_node: 14\nnum_samples: 1762\nnum_features: 14\nX_train: (1762, 14)\nbest validation error rate (epoch 4): 0.25294215150008287\n\ntest error rate (model of epoch 4): 0.2445291777188329\n\ntest accuracy (model of epoch 4): 0.7554708222811671\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -1.3140279\nmax_wx: 4.0102453\nfeature_bias: -0.41402790546417156\ncluster_centres_size: 2\nNo of wts in each cluster: [83, 164]\ncluster_no[label_max_clus]: 164\nlabel_max_clus: 1\ndataset_left: 182\ndataset_right: 1580\nno of 0s in left: 129\nno of 1s in left: 53\nno of 0s in right: 1473\nno of 1s in right: 107\ncurr_info_gain: 0.009252354238736438\nbest_split_info_gain: 0.009252354238736438\nbest_info_gain: 0.009252354238736438\ncurr_depth: 4\ncurr_node: 29\nnum_samples: 182\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 4\ncurr_node: 30\nnum_samples: 1580\nnum_features: 14\nX_train: (1580, 14)\nbest validation error rate (epoch 17): 0.24912978617603182\n\ntest error rate (model of epoch 17): 0.23839522546419098\n\ntest accuracy (model of epoch 17): 0.761604774535809\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -0.061455525\nmax_wx: 4.362712\nfeature_bias: -0.06145552545785904\ncluster_centres_size: 6\nNo of wts in each cluster: [78, 13, 6, 248, 122, 11]\ncluster_no[label_max_clus]: 248\nlabel_max_clus: 3\ndataset_left: 2\ndataset_right: 1578\nno of 0s in left: 2\nno of 1s in left: 0\nno of 0s in right: 1471\nno of 1s in right: 107\ncurr_info_gain: 1.1625359021272264e-05\nbest_split_info_gain: 1.1625359021272264e-05\nbest_info_gain: 1.1625359021272264e-05\ncurr_depth: 5\ncurr_node: 61\nnum_samples: 2\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 5\ncurr_node: 62\nnum_samples: 1578\nnum_features: 14\nX_train: (1578, 14)\nbest validation error rate (epoch 22): 0.23968175037294878\n\ntest error rate (model of epoch 22): 0.23159814323607428\n\ntest accuracy (model of epoch 22): 0.7684018567639257\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -3.8720376\nmax_wx: 1.7297815\nfeature_bias: 0.9279623508453412\ncluster_centres_size: 11\nNo of wts in each cluster: [16, 149, 11, 111, 57, 5, 5, 8, 16, 5, 5]\ncluster_no[label_max_clus]: 149\nlabel_max_clus: 1\ndataset_left: 690\ndataset_right: 888\nno of 0s in left: 668\nno of 1s in left: 22\nno of 0s in right: 803\nno of 1s in right: 85\ncurr_info_gain: 0.0020054803185693276\nbest_split_info_gain: 0.0020054803185693276\nbest_info_gain: 0.0020054803185693276\ncurr_depth: 6\ncurr_node: 125\nnum_samples: 690\nnum_features: 14\nX_train: (690, 14)\nbest validation error rate (epoch 6): 0.2575832918945798\n\ntest error rate (model of epoch 6): 0.2460212201591512\n\ntest accuracy (model of epoch 6): 0.7539787798408488\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -3.135793\nmax_wx: 0.72955066\nfeature_bias: 0.2642070293426544\ncluster_centres_size: 2\nNo of wts in each cluster: [292, 135]\ncluster_no[label_max_clus]: 292\nlabel_max_clus: 0\ndataset_left: 638\ndataset_right: 52\nno of 0s in left: 625\nno of 1s in left: 13\nno of 0s in right: 43\nno of 1s in right: 9\ncurr_info_gain: 0.003249661781048109\nbest_split_info_gain: 0.003249661781048109\nbest_info_gain: 0.003249661781048109\ncurr_depth: 7\ncurr_node: 251\nnum_samples: 638\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 252\nnum_samples: 52\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 6\ncurr_node: 126\nnum_samples: 888\nnum_features: 14\nX_train: (888, 14)\nbest validation error rate (epoch 11): 0.2514503563732803\n\ntest error rate (model of epoch 11): 0.2443633952254642\n\ntest accuracy (model of epoch 11): 0.7556366047745358\n\nwts_list shape: (1500, 14)\neps: 0.3 min_samples: 5\nmin_wx: -0.41539633\nmax_wx: 3.78791\nfeature_bias: -0.11539633274078376\ncluster_centres_size: 3\nNo of wts in each cluster: [189, 17, 3]\ncluster_no[label_max_clus]: 189\nlabel_max_clus: 0\ndataset_left: 22\ndataset_right: 866\nno of 0s in left: 16\nno of 1s in left: 6\nno of 0s in right: 787\nno of 1s in right: 79\ncurr_info_gain: 0.00159189151956457\nbest_split_info_gain: 0.00159189151956457\nbest_info_gain: 0.00159189151956457\ncurr_depth: 7\ncurr_node: 253\nnum_samples: 22\nnum_features: 14\nleaf_value: 0.0\ncurr_depth: 7\ncurr_node: 254\nnum_samples: 866\nnum_features: 14\nleaf_value: 0.0\nTest Accuracy: 0.8413461538461539\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Multiple Classifiers**","metadata":{}},{"cell_type":"code","source":"\ndef evaluate_algorithm(data):\n#     # Load the dataset using the provided fetch function\n#     data_splits = data_fetch_function()\n\n    # Algorithms to evaluate\n    algorithms = [\n        (\"Logistic Regression\", LogisticRegression(max_iter=100)),\n        (\"SVM\", SVC()),\n        (\"Naive Bayes\", GaussianNB()),\n        (\"K-NN\", KNeighborsClassifier()),\n        (\"Relu Neural Networks\", MLPClassifier(activation='relu', hidden_layer_sizes=(100, 50, 25))),\n        (\"Decision Trees\", DecisionTreeClassifier()),\n        (\"Nearest Centroid\", NearestCentroid())\n    ]\n\n    results = []\n\n    # Iterate over each algorithm\n    for algo_name, algo in algorithms:\n        print(f\"Running {algo_name}...\")\n\n        best_acc = 0.0\n        best_params = {}\n\n        # Hyperparameter tuning using GridSearchCV\n        if algo_name == \"Logistic Regression\":\n            param_grid = {\n                'C': [0.001, 0.01, 0.1, 1, 10, 100],\n                'max_iter': [100,1000]\n            }\n        elif algo_name == \"SVM\":\n            # Define parameter grid for SVC\n    #         param_grid = {\n    #             'C': [0.1, 1, 10],\n    #             'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n    #             'gamma': ['scale', 'auto'] + [0.001, 0.01, 0.1, 1, 10],\n    #             'degree': [2, 3, 4, 5],\n    #         }\n            param_grid = {\n                'kernel': ['linear', 'rbf']\n            }\n        elif algo_name == \"Naive Bayes\":\n            param_grid = {\n                'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 1e-04, 1e-03, 1e-02]\n            }\n        elif algo_name == \"K-NN\":\n            # Define parameter grid for KNeighborsClassifier\n            param_grid = {\n                'n_neighbors': [1, 3, 5, 7, 9,15],\n                'weights': ['uniform', 'distance'],\n                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }\n        elif algo_name == \"Relu Neural Networks\":\n            # Define parameter grid for MLPClassifier\n            param_grid = {\n                'hidden_layer_sizes': [(50,50,50), (100, 100,100), (500, 500, 500)],\n                'alpha': [0.0001, 0.001, 0.01],\n                'max_iter': [100, 500],\n            }\n        elif algo_name == \"Decision Trees\":\n            # Define parameter grid for DecisionTreeClassifier\n            param_grid = {\n                'criterion': ['gini', 'entropy'],\n                'splitter': ['best', 'random'],\n                'max_depth': [None, 10, 20, 30,50],\n                'min_samples_split': [2, 5, 10,50],\n                'min_samples_leaf': [1, 2, 4],\n                'max_features': ['sqrt', 'log2']\n            }\n        elif algo_name == \"Nearest Centroid\":\n            # Define parameter grid for NearestCentroid (distance metric)\n            param_grid = {\n                'metric': ['euclidean', 'manhattan']\n            }  \n        else:\n            # Exclude 'priors' from the parameter grid\n            param_grid = {k: v for k, v in algo.get_params().items() if k != 'priors'}\n\n        clf = GridSearchCV(algo, param_grid, cv=2, verbose=1, n_jobs=-1)\n        clf.fit(data.X_valid, data.y_valid)\n\n        best_acc = clf.best_score_\n        best_params = clf.best_params_\n\n        # Train the final model on combined training and validation data\n        model = algo.set_params(**best_params)\n#         X_combined = np.vstack((data.X_train, data.X_valid))\n#         y_combined = np.concatenate((data.y_train,data.y_valid))\n        model.fit(data.X_train,data.y_train)\n\n        # Predict on the test set\n        y_pred = model.predict(data.X_test)\n        test_acc = accuracy_score(data.y_test, y_pred)\n\n        # Store the results\n        results.append({\"Algorithm\": algo_name, \"Best Val Accuracy\": best_acc, \"Test Accuracy\": test_acc})\n\n    # Create a results DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Print the results table\n    print(\"\\nResults Table:\")\n    print(results_df)\n\nDATA_NAME=[\"ADULT\",\"bank_marketing\",\"credit_card_defaults\",\"gamma_telescope\",\"rice_dataset\",\"german_credit_data\",\"spambase_dataset\",\"accelerometer_gyro_dataset\",\"swarm_behaviour\"]#,\"HIGGS\"]\n#open the below comment to run ML algos on UCI datasets\n'''\nfor data_name in DATA_NAME:\n    data = Dataset(data_name)\n    print('classes', np.unique(data.y_test))\n    evaluate_algorithm(data)\n'''\n# # Example usage with different dataset fetch functions\n# evaluate_algorithm(fetch_ADULT)\n# evaluate_algorithm(fetch_rice_dataset)  # Adjust to the appropriate fetch function\n# evaluate_algorithm(fetch_swarm_behaviour)  # Adjust to the appropriate fetch function\n# # Add more evaluate_algorithm calls for other datasets\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T20:54:23.224559Z","iopub.execute_input":"2023-09-28T20:54:23.225221Z","iopub.status.idle":"2023-09-28T20:54:23.248623Z","shell.execute_reply.started":"2023-09-28T20:54:23.225182Z","shell.execute_reply":"2023-09-28T20:54:23.247655Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"\\nfor data_name in DATA_NAME:\\n    data = Dataset(data_name)\\n    print('classes', np.unique(data.y_test))\\n    evaluate_algorithm(data)\\n\""},"metadata":{}}]}]}